{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The NI-HPC Centre is a UK Tier-2 National High Performance Computing (HPC) facility funded by the Engineering and Physical Sciences Research Council (EPSRC) and jointly managed by Queen's University Belfast (QUB) and  Ulster University. The \u00a32M investment from EPSRC will have significant impact for science, by expanding the use of HPC to new pools of talent and new areas of investigation for Northern Ireland. This will be done by building on the strong collaboration between Ulster University and QUB, already established through two major city deal initiatives.</p> <p>To learn more please visit our website.</p>"},{"location":"Application%20Guides/","title":"Application Guides","text":"<p>Here we will provide information on some of the most popular centrally installed applications and software tools on Kelvin2.</p>"},{"location":"Application%20Guides/#anaconda","title":"Anaconda","text":"<p>https://www.anaconda.com/</p> <p>Anaconda is a software that allows the users to manage environments to install local libraries and software, particularly Python and R programming languages, for scientific computing. Thus, greatly easing software management and reducing possible incompatibilities among different tools, libraries or software versions.</p>"},{"location":"Application%20Guides/#installed-versions","title":"Installed versions","text":"Anaconda modules<pre><code>apps/anaconda/2.5.0/bin\napps/anaconda3/2021.05/bin\napps/anaconda3/2022.10/bin\napps/anaconda3/5.2.0/bin\n</code></pre>"},{"location":"Application%20Guides/#usage-notes","title":"Usage notes","text":"Redirecting default installation paths to Scratch directory <p>When installing packages with Anaconda, it is recommended that users redirect the default installation paths to their Scratch directory. This is because the installations typically generate a very large number of small files which can breach the 100k file limit in place on the Home directory. This can be done by modifying the environment variables <code>CONDA_PKGS_DIRS</code> and <code>CONDA_ENVS_PATH</code> as follows:</p> <pre><code>mkdir /mnt/scratch2/users/$USER/conda\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\n</code></pre> Selecting the correct hardware prior to installation <p>When installing packages with Anaconda, it is critical to perform the installation on a node with the appropriate hardware in place. For example, if you intend to run your code using a GPU device, but install that code (or its dependencies) using a node without one, the GPU device may not being recognised at runtime.  This may result in your program crashing, or instead run in the CPU cores by default. In the latter case, your jobs may take significantly longer to complete and also would also cause the requested GPU resources to be sitting idle and unavailable to other users.</p> Complexities in installing packages with Python and Anaconda <p>When installing packages with both Python and Anaconda, there may be complexities related to package version incompatibilities, the proper Python version (maybe an older or newer version is strictly required), package installation order, or the possible necessity to install supporting libraries. The first three \"Usage examples\" below demonstrate the installation of some commonly used applications.</p>"},{"location":"Application%20Guides/#usage-examples","title":"Usage examples","text":"Installing PyTorch and Ray Tune <p>In this example, PyTorch is installed together with Ray Tune. The latter is a tool used for deep learning models optimization as it helps with the evaluation and selection of model hyperparameters (e.g., number of layers, neurons per layer, selection between different transfer or optimization functions, etc.)</p> Pytorch-Raytune install<pre><code>srun -p k2-gpu-interactive -N 1 -n 1 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nmodule load libs/nvidia-cuda/11.7.0/bin\nconda create --name py39torchRayA100 python=3.9\nsource activate py39torchRayA100\n(py39torchRayA100) conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n(py39torchRayA100) python3\n(py39torchRayA100) conda install pytorch-lightning -c conda-forge\n(py39torchRayA100) conda install -c conda-forge \"ray-air\"\n(py39torchRayA100) conda deactivate\n</code></pre> <p>Line 1 above shows the use of GPU slice partitions: <code>--gres gpu:1g.10gb:1</code>. As currently there are available 28 slices in Kelvin2, this partition is much less busy than the other GPU partitions, and at the same time would allow the users to install software in the A100 GPU devices, which would guarantee backward compatibility with GPU devices.</p> <p>Lines 2-5 setup the environment variables, create an Anaconda environment named <code>py39torchRayA100</code> while installing Python version 3.9, and activate the environment. This Python version is strictly required here as current Ray Tune installation version may crash with newer Python versions than 3.10.</p> <p>The next lines 6-10 are needed to install the required tools, particularly Ray Tune is installed in line 9. Here, the package \"ray-air\" contains most of the provided functionality by Ray Tune (Data, Train, Tune, Serve, etc.).</p> <p>After installation, users must check that PyTorch can correctly utilize the GPU resources available. For example, users must see something similar to the following output when running the PyTorch's functions:</p> Testing that torch can see GPU resources<pre><code>source activate py39torchRayA100\n(py39torchRayA100) python3\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.cuda.is_available()\nTrue\n&gt;&gt;&gt; torch.cuda.current_device()\n0\n&gt;&gt;&gt; torch.cuda.device(0)\n&lt;torch.cuda.device object at 0x7fec6d471850&gt;\n&gt;&gt;&gt; torch.cuda.get_device_name(0)\n'NVIDIA A100-SXM4-80GB MIG 1g.10gb'\n&gt;&gt;&gt; exit()\n(py39torchRayA100) conda deactivate\n</code></pre> Installing tensorflow-gpu and Ray Tune <p>For installing tensorflow-gpu and Ray Tune, follow these instructions:</p> Tensorflow-Raytune install<pre><code>srun -p k2-gpu-interactive -N 1 -n 1 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nmodule load libs/nvidia-cuda/11.7.0/bin\nconda create --name tensorflowRayA100 python=3.9\nsource activate tensorflowRayA100\n(tensorflowRayA100) conda install -c anaconda tensorflow-gpu\n(tensorflowRayA100) conda install -c conda-forge \"ray-air\"\n(tensorflowRayA100) conda deactivate\n</code></pre> <p>Users also must test that tensorflow can correctly see the GPU resources. For that purpose, after installing the software, use the following commands:</p> Testing that tensorflow can see GPU resources<pre><code>source activate tensorflowRayA100\n(tensorflowRayA100) python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n(tensorflowRayA100) conda deactivate\n</code></pre> Installing and using Bindsnet <p>In this example, users can overview how to install Bindsnet in Kelvin2:</p> Bindsnet install<pre><code>srun -p k2-gpu-interactive -N 1 -n 1 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nconda create --name bindsnet\nexport PATH=/mnt/scratch2/users/$USER/conda/envs/bindsnet/bin/:$PATH\nsource activate bindsnet\n(bindsnet) conda install python=3.10\n(bindsnet) python3 -m pip install git+https://github.com/BindsNET/bindsnet.git\n(bindsnet) python3 -m pip show bindsnet\n(bindsnet) python3 -m pip install pytest\n(bindsnet) conda deactivate\n</code></pre> <p>The apparent simplicity of above instructions hides some complexities. For example, the declaration of the \"PATH\" environment variable in line 4 may be neccesary when PyPi/pip is combined with Anaconda, as some of the installed packages may still be missing after installation, unexpectedly. Also, in line 6, Python's version 3.10 was installed as Bindsnet most recent version at this moment seems not to be compatible with older Python versions.</p> <p>Finally, when installing Bindsnet in line 7, it automatically finds and installs all needed dependencies, including the compatible versions for torch, torchvision, etc. Therefore, we do not recommend to install any package before bindsnet, otherwise bindsnet installation can fail. Then, in line 8, the command <code>python3 -m pip show bindsnet</code> allows to get the physical path where Bindsnet is installed, which may be necessary to access some folders with testing examples and scripts. For the same reasons, users may want to install the package \"pytest\", as it is done in line 9.</p> <p>As Bindsnet will install automatically PyTorch as one of its dependencies, we strongly recommend to test that PyTorch can see the GPU resources, as discussed in the Installing Pytorch and Ray Tune example.</p> <p>The following is an example of sbatch script that uses the installed Bindsnet package, which can be launched from command line using the command <code>sbatch bindsnet_example.sh</code>. Of course, users must have first to prepare their python codes, with the main file name provided in line 19. </p> bindsnet_example.sh<pre><code>#!/bin/bash\n#SBATCH --job-name=nnet\n#SBATCH -N 1\n#SBATCH -n 4\n#SBATCH --mem=40G\n#SBATCH --partition=k2-gpu\n#SBATCH --gres gpu:v100:1\n##SBATCH --gres gpu:a100:1\n#SBATCH --time=03:00:00\n#SBATCH --output=nnet_%j.log\n\nmodule load apps/anaconda3/2021.05/bin\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\nexport PATH=/mnt/scratch2/users/$USER/conda/envs/bindsnet/bin/:$PATH\n\nsource activate /mnt/scratch2/users/$USER/conda/envs/bindsnet\n\npython3 &lt;code_file_name&gt;.py\n</code></pre> Installing R Packages (e.g. HOMER) with Anaconda <p>To create an environment on the user's scratch folder to install a particular version of R, follow these instructions:</p> R installation in an Anaconda environment<pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\nconda create -n R412env -c conda-forge r-base=4.1.2\n</code></pre> <p>Here, the preferred version (4.1.2) is specified in line 5 with the argument \"r-base=4.1.2\". For other versions, check the Anaconda R's online documentation, for example here or here for more information.</p> <p>Installation Example: HOMER</p> <p>The following example illustrates how to use this environment for installing a particular R's package HOMER, proceeding from a clean/new Kelvin2 terminal connection.</p> <pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\nsource activate R412env\n(R412env)$ conda config --add channels defaults\n(R412env)$ conda config --add channels bioconda\n(R412env)$ conda config --add channels conda-forge\n(R412env)$ conda install -c bioconda samtools r-essentials bioconductor-deseq2 bioconductor-edger\n(R412env)$ conda install homer\n(R412env)$ conda update homer\n(R412env) conda deactivate\n</code></pre> <p>Users should notice that when working with R inside an Anaconda environment, the libraries must be installed with the command <code>conda install ...</code>. Whereas, the packages can be installed either in the same way by using conda install, or from the R's command line; for instance, using the function <code>installed.packages()</code>. </p> Jupyter Notebook with TensorFlow in Anaconda <p>Here, we present instructions to install and use Jupyter notebook in Kelvin2, in the case for training deep learning networks with tensorflow-gpu.</p> Jupyter notebook installation<pre><code>srun -p k2-gpu-interactive -N 1 -n 4 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\n#srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nconda create -n tf-gpu tensorflow-gpu\n# export PATH=/mnt/scratch2/users/$USER/conda/envs/tf-gpu/bin/:$PATH\nsource activate tf-gpu\n(tf-gpu) conda install -c anaconda jupyter\n(tf-gpu) conda deactivate\n</code></pre> <p>If the user does not need to use GPU resources, for example, the plan is to run on CPU cores, then <code>srun</code> instruction in line 2 must be used, instead of line 1, to get an interactive session in CPU nodes for the installation. Also, ignore the installation of tensorflow part if the only purpose is to install Jupyter notebook to be used with other packages, such as Pandas or NumPy. In that case, just create the environment in line 4 with the following instruction: <code>conda create -n &lt;environment_name&gt;</code>.</p> <p>Finally, to access Jupyter notebook remotely from the (local) browser in the user's PC/laptop, the user must launch a Jupyter server connection and create a tunnel to it. To launch the server, run these commands:</p> <pre><code>srun -p k2-gpu-interactive -N 1 -n 4 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\n#srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nsource activate tf-gpu\n(tf-gpu) jupyter notebook --ip $(ip addr show em1 | grep 'inet ' | awk '{print $2}' | cut -d/ -f1) --no-browser\n</code></pre> <p>Jupyter notebook has to run in a compute node. That is why first need to allocate a compute node with <code>srun</code> command as shown in line 1 or 2 for a GPU or CPU node. Here, the server is lauched in line 5. Some critical output is printer, mainly the IP and port number, which should be annotated for using later to create the tunnel.</p> <p>For the last part, open a local terminal in the user's PC/laptop, and enter the following instructions:</p> <pre><code>ssh -p 55890 -i /path/to/kelvin/key &lt;user_name&gt;@login.kelvin.alces.network -NL 8888:10.10.15.3:8888\n</code></pre> <p>This establishes the tunnel using <code>ssh</code> command, where it has been assumed that the IP and port number annotated above are \"10.10.15.3\" and \"8888\", respectively.</p> <p>If not errors are reported during the execution of these commands, and the terminal looks like hanging out, then everything is ok, and the last step is to open a local browser and enter the adress \"http://127.0.0.1 ...\", which must have been shown in the output when the server connection was created.</p>"},{"location":"Application%20Guides/#ansys","title":"Ansys","text":"<p>https://www.ansys.com</p> <p>\"For more than 50 years, Ansys software has enabled innovators across industries to push boundaries with the predictive power of simulation. From sustainable transportation and advanced semiconductors, to satellite systems and life-saving medical devices, the next great leaps in human advancement will be powered by Ansys.\" - Ansys Company Information</p>"},{"location":"Application%20Guides/#installed-versions_1","title":"Installed versions","text":"<pre><code>apps/ansys/2023.1\n</code></pre>"},{"location":"Application%20Guides/#usage-notes_1","title":"Usage notes","text":"Licensing <p>Ansys is a licensed software. In order to use it, users have to be registered in the license server. If you are not already included in the license server, you must contact the person in charge of it:</p> <p>QUB: TBA</p> <p>Ulster: TBA</p> <p>When the module is loaded, license parameters for QUB are loaded by default. Ulster users should change these license parameters in their batch script or interactive session. Contact the person in charge of the license for details.</p> User Manual <p>The user manual for Ansys is not publicly available, and only licensed users can access to it. To access to this material, you must register on the Ansys website.</p>"},{"location":"Application%20Guides/#usage-examples_1","title":"Usage examples","text":"Ansys Fluent batch script example <pre><code>#!/bin/bash\n\n#SBATCH --job-name=myfluentjob\n#SBATCH --output=myoutput.out\n#SBATCH --error=myerror.err\n#SBATCH --nodes=1\n#SBATCH --ntasks=32\n#SBATCH --partition=k2-hipri\n#SBATCH --mem=100G\n\nmodule load apps/ansys/2023.1\n\n## QUB's license, already loaded with the module\n#export ANSYSLI_SERVERS=2325@143.117.212.118\n#export ANSYSLMD_LICENSE_FILE=1055@143.117.212.118\n\n## Ulster's license\n#export ANSYSLI_SERVERS=2325@193.61.145.219\n#export ANSYSLMD_LICENSE_FILE=1055@193.61.145.219\n\n# Set architecture of the CPU (in this case amd64) and environment variables\nexport FLUENT_ARCH=lnamd64\nexport FL_TMPDIR=$SCRATCH/tmp\n\n# Create our hosts file \nsrun hostname -s | sort &gt; hosts.$SLURM_JOB_ID.txt\n\n#Run Ansys Fluent.\nfluent 3ddp -g -t$SLURM_NTASKS -pinfiniband -mpi=openmpi -cnf=hosts.$SLURM_JOB_ID.txt -i my_fluent_input &gt; my_fluent_output.res\n</code></pre>"},{"location":"Application%20Guides/#matlab","title":"MATLAB","text":"<p>https://uk.mathworks.com/products/matlab.html</p> <p>\"MATLAB is a programming platform designed specifically for engineers and scientists to analyze and design systems and products that transform our world. The heart of MATLAB is the MATLAB language, a matrix-based language allowing the most natural expression of computational mathematics.\" - What is MATLAB?</p>"},{"location":"Application%20Guides/#installed-versions_2","title":"Installed versions","text":"<pre><code>matlab/R2019a\nmatlab/R2020b\nmatlab/R2022a\n</code></pre>"},{"location":"Application%20Guides/#usage-examples_2","title":"Usage examples","text":"Interactive mode (CLI) on a CPU compute node <p>One way to run MATLAB interactively is to request a compute node with the <code>srun</code> command. For example, request 1 compute node and 10 cores in \"k2-hipri\" partition:</p> <pre><code>srun -p k2-hipri -N 1 -n 10 --mem=10G --time=1:00:00 --pty bash\nmodule load matlab/R2022a\nmatlab -nosplash -nodisplay\n</code></pre> <p>Then, inside MATLAB, notice that calling the function <code>feature</code> (line #1 below) must show that exactly 10 CPU cores are available, corresponding to the number of cores allocated above with the <code>srun</code> command. Next, the codes launches the parallel pool (<code>parpool</code>) for the \"local\" cluster requesting the same amount of workers as the cores allocated in this example.</p> <pre><code>feature('numcores')\np = parpool('local', 10) % 10 cores requested in this example\n</code></pre> <p>More robust, the number of workers can be read and set automatically to launch the parallel pool:</p> <pre><code>num_workers = str2double(getenv('SLURM_CPUS_ON_NODE'))\np = parpool('local', num_workers)\n</code></pre> <p>Finally, you can run your MATLAB parallel code, which must include a \"parfor\" loop. For example, the following code illustrates the use of <code>parfor</code> and Monte Carlo simulation to calculate an approximate value for \\( \\pi \\), using the formula (line #7):</p> \\[ \\pi \\approx \\lim_{N_{MC} \\to \\infty} {4 \\sum_{n=1}^{N_{MC}} I(x_i^2 + y_i^2 &lt; 1.0) \\over N_{MC}}; x_i, y_i \\sim \\mathcal{U}(0,1), \\] <p>where the symbol \\( \\mathcal{U}(0,1) \\) represents the random uniform distribution for the indicated interval and \\( I(boolean) \\) is an indicator function.</p> Matlab code for parfor demonstration<pre><code>N = 1e6;\nout = zeros(1, num_workers);\nparfor i = 1:num_workers\n    xy = rand(N,2);\n    out(i) = sum(sum(xy.^2,2)&lt;=1);\nend\nmypi = 4*sum(out)/(N*num_workers)\n</code></pre> <p>At the end of the parallel computations, the allocated parpool within MATLAB can be released with the following command:</p> <pre><code>delete(gcp('nocreate'))\n</code></pre> Interactive mode (GUI) on a CPU compute node <p>First, get a compute node and launch vnc server from the node, for example:</p> <pre><code>srun -p k2-hipri -N 1 -n 6 --mem=10G --time=1:00:00 --pty bash\nvncserver\n</code></pre> <p>In this case, let us assume that the output of <code>vncserver</code> is</p> <p>New 'node117.pri.kelvin2.alces.network:1 (jsan)' desktop is node117.pri.kelvin2.alces.network:1</p> <p>Starting applications specified in /users/jsan/.vnc/xstartup Log file is /users/jsan/.vnc/node117.pri.kelvin2.alces.network:1.log</p> <p>Then, open a local terminal and launch a forward tunnel to the compute node by following these steps:</p> <ol> <li>Go to the directory which contains the Kelvin key in your PC/laptop <pre><code>cd /drives/c/Users/jsan/.ssh\n</code></pre></li> <li>Create the tunnel (in this example illustrated with the command below, all input sent via port 5903 on your local host is being forwarded via port 5901 to the compute node \"node117.pri.kelvin2.alces.network\". If the <code>vncserver</code> output above were \"node117.pri.kelvin2.alces.network:7\", then the port number will be 5907 instead of 5901. Clearly, users must replace the username \"jsan\" and the key's filename by the corresponding information for their accounts) <pre><code>ssh -L 5903:node117.pri.kelvin2.alces.network:5901 -p 55890 -i ./kelvin-key jsan@login.kelvin.alces.network\n</code></pre></li> <li>Connect to the tunnel using your installed VNC application (the example shown in the figure below uses TurboVNC in Windows OS. More details or troubleshooting can be found in the VNC session) </li> <li>Once in the opened terminal for the connected compute node, launch the MATLAB application: <pre><code>module load matlab/R2022a\nmatlab\n</code></pre></li> </ol> <p>This time MATLAB GUI will be opened as shown in the figure below. For a better experience, use the \"full screen\" button in the VNC toolbar, and use the combination keys Ctrl+Alt+Shift + F to escape from the full screen mode. Note also that using the instruction <code>feature('numcores')</code> inside the MATLAB GUI session shows correctly the number of allocated CPU cores in the compute node (CPU cores equal to 6 in this example).</p> <p></p> MATLAB script used in following GPU examples <p>The following code <code>matlab_gpu_example.m</code> is to be used in the following GPU examples</p> matlab_gpu_example.m<pre><code>% The Mandelbrot algorithm iterates over a grid of real and imaginary parts.\n% The following code defines the number of iterations, grid size, and grid limits.\nmaxIterations = 500;\ngridSize = 1000;\nxlim = [-0.748766713922161, -0.748766707771757];\nylim = [ 0.123640844894862,  0.123640851045266];\n\n% Use the gpuArray function to transfer data to the GPU and create a gpuArray object.\nx = gpuArray.linspace(xlim(1), xlim(2), gridSize);\ny = gpuArray.linspace(ylim(1), ylim(2), gridSize);\n\n% Many Matlab functions support gpuArrays and run directly on the GPU (e.g., meshgrid)\n% The function \"ones\", below, create an array of ones directly on the GPU\n[xGrid,yGrid] = meshgrid(x,y);\nz0 = xGrid + 1i*yGrid;\ncount = ones(size(z0), 'gpuArray');\n\n% The code below implements the Mandelbrot algorithm, fully running on the GPU\nz = z0;\nfor n = 0:maxIterations\n    z = z.*z + z0;\n    inside = abs(z) &lt;= 2;\n    count = count + inside;\nend\ncount = log(count);\n\n% Plot the results\nimagesc(x, y, count);\ncolormap([jet(); flipud(jet()); 0 0 0]);\naxis off;\n</code></pre> Interactive mode (GUI) on a GPU compute node <p>First, get a GPU node and launch vnc server from the node, for example:</p> <pre><code>srun -p k2-gpu-interactive -N 1 -n 4 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nvncserver\n</code></pre> <p>Then (as with the instructions in the 'Interactive mode (GUI) on a CPU compute node' example above),</p> <ol> <li>Open a local terminal and launch a forward tunnel to the GPU node;</li> <li>Use a VNC application to connect to the tunnel</li> <li>Load and launch the MATLAB software.</li> </ol> <p>To test the use of the GPU device within MATLAB code, copy and paste the lines from the <code>matlab_gpu_example.m</code> script file above.</p> <p>The following results should appear when you run this script in the GUI that is running on the cluster's GPU, as per below. As shown in the MATLAB command window, the output of calling the function <code>whos</code> reveal the many variables (gpuArray objects) that are still allocated on the GPU.</p> <p></p> Batch script example (GPU) <p>The most convenient way to use the cluster resources may be to perform MATLAB calculations in background, using <code>sbatch</code> functionality instead of <code>srun</code>. This is critical, mainly for analysis where calculations may take several hours or days. To demonstrate this, simply copy and paste the following \"sbatch\" script, called <code>gpu_example.sh</code>, as an example which relies on the same script prepared above to calculate the Mandelbrot solution.</p> <p>gpu_example.sh<pre><code>#!/bin/bash\n\n#SBATCH --job-name=test\n#SBATCH -N 1\n#SBATCH -n 4\n#SBATCH --mem=20G\n#SBATCH --time=00:10:00\n#SBATCH --partition=k2-gpu\n#SBATCH --gres=gpu:1g.10gb:1\n#SBATCH --output=test_%j.log\n\nmodule load matlab/R2022a\n\n# Ulster University (UU) users must use UU's licence by declaring\n# (removing comments) these environmet variables:\n#export MLM_LICENSE_FILE=27000@193.61.190.229\n#export LM_LICENSE_FILE=27000@193.61.190.229\n\nmatlab -nosplash -nodisplay -r \"matlab_gpu_example; saveas(gcf, 'Mandelbrot'); exit;\"\n</code></pre> In the code above, line #19, MATLAB is called in \"nosplash\" and \"nodisplay\" mode to execute the <code>matlab_gpu_example.m</code> script. Besides, note that the code will run in background, i.e., without display, therefore it must use some MATLAB function like <code>saveas</code>, as shown in the code, which saves the graphical output. As a result, the MATLAB file that must be listed now in the working directory, <code>Mandelbrot.fig</code>, should contain the visual results. Finally, to run this \"sbatch\" script, run the following command in your terminal.</p> <pre><code>sbatch gpu_example.sh\n</code></pre>"},{"location":"Application%20Guides/#paraview","title":"ParaView","text":"<p>https://www.paraview.org/</p> <p>\"ParaView is the world\u2019s leading open source post-processing visualization engine. It integrates with your existing tools and workflows, allowing you to build visualizations to analyze data quickly. With its open, flexible, and intuitive user interface, you can analyze extremely large datasets interactively in 3D or programmatically using ParaView\u2019s batch processing.\" - About ParaView</p>"},{"location":"Application%20Guides/#installed-versions_3","title":"Installed versions","text":"<pre><code>apps/paraview/5.10.0-rc1/bin\napps/paraview/5.4.1/bin\napps/paraview/5.6.1/bin\napps/paraview/5.8.1/bin\napps/paraview/5.9.0/bin\n</code></pre>"},{"location":"Application%20Guides/#usage-examples_3","title":"Usage examples","text":"Paraview Server on Kelvin2 with visualisation on local machine <p>A recommended method to run Paraview on Kelvin-2, it is to start a PV server on Kelvin-2, and to use your local machine to visualize the graphical interface. In this case, you need an installation of Paraview in your local machine as well.</p> <p>With the following method, Paraview can be run in parallel, and use the local machine to work with the graphical interface. It is necessary to set a tunnel through the Kelvin-2 login node.</p> <p>As example, we are going to explain how to set the tunnel and connect with a computer outside the QUB campus. If the local machine is insithe the QUB campus, you have to remove the specification of the port <code>-p</code>, the identification private-key file <code>-i</code>, and to change the IP name to <code>kelvin2.qub.ac.uk</code>.</p> <p>Step 1. Run pvserver in a computing node of Kelvin-2.</p> <p>Open the session in Kelvin-2 as usual, and then open an interactive session. <code>&lt;N&gt;</code> will be the number of cores you require for your parallel job on Paraview.</p> <pre><code>$ srun --pty --partition=k2-hipri --ntasks=&lt;N&gt; --mem-per-cpu=2G /bin/bash\n</code></pre> <p>Then, you are transferred to a computing node.</p> <p>Now load the necessary modules, for paraview, and for OpenMPI.</p> <pre><code>[&lt;uid&gt;@nodeNNN [kelvin2] ~]$ module load apps/paraview/5.8.1/bin\n[&lt;uid&gt;@nodeNNN [kelvin2] ~]$ module load mpi/openmpi/4.0.4/gcc-9.3.0+ucx-1.8.0\n</code></pre> <p>Run the server with the command</p> <pre><code>[&lt;uid&gt;@nodeNNN [kelvin2] ~]$ mpirun -np &lt;N&gt; pvserver --force-offscreen-rendering --server-port=&lt;YYYY&gt;\n</code></pre> <p>Where <code>&lt;uid&gt;</code> is your Kelvin-2 username, <code>&lt;N&gt;</code> is the number of MPI tasks you want to run Paraview, and <code>&lt;YYYY&gt;</code> is a port number your choice.</p> <p>Step 2. Create the tunnel to the port <code>&lt;YYYY&gt;</code>, through the port <code>&lt;XXXX&gt;</code> of your local machine.</p> <p>In a different shell on your local machine, open the tunnel with the command:</p> <pre><code>(local)$ ssh -L &lt;XXXX&gt;:nodeNNN:&lt;YYYY&gt; -p 55890 -i ~/.ssh/my-kelvin-key &lt;uid&gt;@login.kelvin.alces.network\n</code></pre> <p>Where <code>&lt;uid&gt;</code> is your Kelvin-2 username, <code>&lt;YYYY&gt;</code> is the port number you let open in the former step, and <code>&lt;XXXX&gt;</code> is a different port number your choice on your local machine. <code>nodeNNN</code> is the computing node that the <code>srun</code> command directed you in the former step.</p> <p>Step 3. Open paraview in your local machine.</p> <p>Inside the application click: File - Connect - Give a name to the session, e. g. \"remote machine\" - In \"Server Type\" choose: Client / Server - host: localhost - Port: <code>&lt;XXXX&gt;</code>  (from step 2)</p> <p>Configure - Startup Type: Manual - Press Save</p> <p>Connect Once you created the client the first time, you can recover it the next time you open Paraview, you do not need to create it again, and you can connect directly in the step 3.</p> <p></p>"},{"location":"Application%20Guides/#python","title":"Python","text":"<p>https://www.python.org/</p> <p>Python is a high-level, multi-purpose programming language that support several paradigms such as structural, functional and object-oriented programming, together with garbage collection, in order to support code readability and fast development. With the provision of multiple specialized libraries, Python has become one of the most preferred languages for scientific programming, including machine learning and artificial intelligence, as well as bioinformatics among many other applications.</p>"},{"location":"Application%20Guides/#installed-versions_4","title":"Installed versions","text":"Python modules<pre><code>apps/python/2.7.8/gcc-4.8.5\napps/python3/3.10.0/gcc-4.8.5\napps/python3/3.10.5/gcc-9.3.0\napps/python3/3.4.3/gcc-4.8.5\napps/python3/3.5.2/gcc-4.8.5\napps/python3/3.6.4/gcc-4.8.5\napps/python3/3.7.4/gcc-4.8.5\napps/python3/3.7.9/gcc-10.2.0\napps/python3/3.8.5/gcc-4.8.5\n</code></pre>"},{"location":"Application%20Guides/#usage-notes_2","title":"Usage notes","text":"Anaconda environments <p>It is often recommended to use Anaconda environments to install Python packages rather than pip. This is to reduce the risk of installation problems like package incompatibilities and the user not having write permissions to Kelvin2 system paths.</p> Installing pip packages in Scratch Directory <p>It is recommended that users declare/modify the environment variables \"PATH\" and \"PYTHONPATH towards locations in the Shared Scratch directory in order to preserve space/quota in the Home directory.</p> <p><pre><code>mkdir /mnt/scratch2/users/$USER/gridware\nexport PATH=/mnt/scratch2/users/$USER/gridware/bin/:$PATH\nexport PYTHONPATH=/mnt/scratch2/users/$USER/gridware/site-packages/:$PYTHONPATH\n</code></pre> This creates and uses the folder \"gridware\" in users' Scratch directory, so the pip install will be redirected to the Scratch directory instead of the Home directory.</p> Default Python version on Kelvin2 <p>On internet blogs/forums, users will often find the recommendation <code>pip install &lt;package name&gt;</code> to install a particular tool. In this case, for the same install in Kelvin2, it is recommended to precede the command with \"python3 -m\" because by default pip will refer to the Python 2.7 version, which is always available from command line in Kelvin2. That is, always use <code>python3 -m pip install &lt;package name&gt;</code> to install the package.</p>"},{"location":"Application%20Guides/#usage-examples_4","title":"Usage examples","text":"Installing reportseff in Home directory <p>The next code snippet illustrates simply how to load a module and install a package in the default location (gridware folder located in the users' home folder). This approach is only recommended for small installs.</p> <pre><code>module load apps/python3/3.10.5/gcc-9.3.0\npython3 -m pip install reportseff\nreportseff -u $USER\n</code></pre> <p>Reportseff is a very usefull tool to monitor the efficient utilization of cluster resources (time, RAM, CPU/GPU) that may be critical for your work. It should be used in addition to <code>sacct</code> command, because it helps to optimize your jobs which may translates on significantly lower queue times. For instance, the reportseff's outcome below shows some job statistics (greed/red colour codes highlight good/poor resources utilization) for selected jobs.</p> <p></p>"},{"location":"Application%20Guides/#r","title":"R","text":"<p>https://www.r-project.org/</p> <p>R is an open-source and free software which provides a programming language designed purposedly for statistical analyses. It is also highly preferable for data mining tasks and machine learning analysis. </p>"},{"location":"Application%20Guides/#installed-versions_5","title":"Installed versions","text":"<pre><code>apps/R/3.2.1/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.2.5/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.3.2/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.4.2/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.5.1/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.6.1/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.6.3/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/4.0.4/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/4.1.0/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/4.1.2/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/4.1.3/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\napps/R/4.2.2/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\napps/R/4.3.0/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\nR/4.1.0/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\nR/4.1.2/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\n</code></pre>"},{"location":"Application%20Guides/#usage-notes_3","title":"Usage notes","text":"Setting R environment <p>By default, installing packages in R will fill up the users' quota (~50 GB hard disk + 100,000 files limit); therefore, it is advisable that the users setup an installation path for the packages. Below, the default install location is redirected to the user's scratch folder. The most transparent way to perform this operation is to create the file \".Renviron\" in the home folder, which will keep the necessary R's environment variables to setup correctly the settings.</p> Setup R environment<pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmkdir /mnt/scratch2/users/$USER/R\nmkdir /mnt/scratch2/users/$USER/R/lib\ncd ~\necho \"R_LIBS_USER=/mnt/scratch2/users/$USER/R/lib\" &gt; .Renviron\necho \"R_LIBS=/mnt/scratch2/users/$USER/R/lib\" &gt;&gt; .Renviron\necho \"PKG_CONFIG_PATH=/mnt/scratch2/$USER/R/lib/pkgconfig\" &gt;&gt; .Renviron\necho \"\" &gt;&gt; .Renviron\nmodule load apps/R/4.3.0/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\nR\n&gt; .libPaths()\n&gt; quit()\n</code></pre> <p>In line 1, a compute node is requested to perform packages installation and work with R. Do not use login nodes to perform these operations. Lines 2-3 create the needed R's library folder. If they are not created, environment initialization may fail to be setup correctly. This operation needs to be done only one time at the very beginning of your R utilization in Kelvin2. It is neccesary to create \".Renviron\" in the home folder which is guaranteed in line 4. Then, the \".Renviron\" file is prepared in line 5-8. Notice, in line 8, this file must end with a newline character. If not, the instruction in the last line will be ignored without warning nor error. The environment file is also setup once, or every time the user wish to change the default install and library folders. Once the R's module is loaded and R is lauched in lines 9-10, the test of running the R's function <code>.libPaths()</code> must show correctly the new library path (line 11).</p>"},{"location":"Application%20Guides/#usage-examples_5","title":"Usage examples","text":"Installing R packages, e.g. BiocManager, DESeq2 <p>After setting up correctly the \".Renviron\" file as presented above, installation of R's packages can proceed inside the R's application as follows:</p> <pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load libpng/16\nmodule load apps/R/4.3.0/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\nexport HDF5_USE_FILE_LOCKING='FALSE'\nR\n&gt; if (!requireNamespace('BiocManager', quietly = TRUE))\n&gt; install.packages('BiocManager')\n&gt; BiocManager::install('DESeq2')\n&gt; installed.packages()\n&gt; quit()\n</code></pre> <p>Here, some issues may arise, such as that packages will not be installed correctly if some dependencies, libraries, or settings are not guaranteed. Therefore, the commands in lines 2, 4 are necessary in the present example. After that, the installation of the packages \"BiocManager\" and \"DESeq2\" is most straightforwardly done as shown in lines 7-8. Finally, the call to <code>installed.packages()</code> in line 9 will show all the installed R's packages.</p> <p>When required libraries are not available in Kelvin2 module system, the users must contact the RSE Kelvin2 team. Otherwise, they can proceed to create an Anaconda environment, where installation of the R's preferred version, libraries and packages can be performed.</p>"},{"location":"Application%20Guides/#singularity","title":"Singularity","text":"<p>In Kelvin-2, we use Singularity to run containers. The main advantage of Singularity is that it does not require root privileges to install the containers. Because of that, it is the most commonly used container in HPC systems.</p>"},{"location":"Application%20Guides/#installed-versions_6","title":"Installed versions","text":"<pre><code>apps/singularity/3.10.0\napps/singularity/3.4.2\n</code></pre>"},{"location":"Application%20Guides/#usage-examples_6","title":"Usage examples","text":"<p>More detail about using singularity and containers on Kelvin-2 can be found in the online seminar</p> Running a Docker image on Singularity <p>You need the docker image file <code>mydockerimage.img</code> as a tarball <code>mytarball.tar</code></p> <p>Go to a compute node <pre><code>srun --pty --partition=k2-hipri --ntasks=1 --mem-per-cpu=2G bash\n</code></pre></p> <p>Load the module <pre><code>module load apps/singularity/3.4.2\n</code></pre></p> <p>To convert the tarball to singularity, first go to the directory where to tarball is located</p> <pre><code>cd /path/to/tarball\n</code></pre> <p>Convert the tarball:</p> <pre><code>singularity build --sandbox mytarball docker-archive://mytarball.tar\n</code></pre> <p>Execute the image</p> <pre><code>singularity shell myimage\nsingularity exec myimage mycommand\nsingularity run myimage\n</code></pre> <p>Create the tarball from a Docker image</p> <p>These steps should be done in your local computer, where you have docker. Once you create the tarball, you have to copy it to Kelvin-2 and create the Singularity image in Kelvin-2.</p> <pre><code>&lt;my_local_machine&gt;$ docker save mydockerimage -o mytarball.tar\n</code></pre>"},{"location":"Applications/","title":"Applications","text":"<p>Here we will provide information on some of the most popular centrally installed applications and software tools on Kelvin2.</p>"},{"location":"Applications/#matlab","title":"Matlab","text":"<p>MATLAB is a proprietary multi-paradigm programming language and numeric computing environment developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages.</p> <p>How to load the application:</p> <pre><code>module load matlab/R2022a\n</code></pre> <p>If X11 forwarding is enabled, running the command <code>matlab</code> will open the Matlab's GUI (it works in Mobaxterm). Otherwise, launch Matlab in console mode as follows:</p> <pre><code>matlab -nosplash -nodisplay\n</code></pre> <p>However, please, DO NOT RUN MATLAB NOR ANY OTHER SOFTWARE ON LOGIN NODES. </p>"},{"location":"Applications/#running-matlab-interactively-srun-parfor-cpus","title":"Running Matlab interactively: srun + parfor (CPUs)","text":"<p>One of the two correct ways to run Matlab is to request a compute node (interactively) with the <code>srun</code> command.</p> <p>For example, request 1 compute node and 10 cores in \"k2-hipri\" partition:</p> <pre><code>srun -p k2-hipri -N 1 -n 10 --mem=10G --time=1:00:00 --pty bash\nmodule load matlab/R2022a\nmatlab -nosplash -nodisplay\n</code></pre> <p>Then, inside Matlab, notice that calling the function <code>feature</code> (line #1 below) must show that exactly 10 CPU cores are available, corresponding to the number of cores allocated above with the <code>srun</code> command. Next, the codes launches the parallel pool (<code>parpool</code>) for the \"local\" cluster requesting the same amount of workers as the cores allocated in this example.</p> <pre><code>feature('numcores')\np = parpool('local', 10) % 10 cores requested in this example\n</code></pre> <p>More robust, the number of workers can be read and set automatically to launch the parallel pool:</p> <pre><code>num_workers = str2double(getenv('SLURM_CPUS_ON_NODE'))\np = parpool('local', num_workers)\n</code></pre> <p>Finally, you can run your Matlab parallel code, which must include a \"parfor\" loop. For example, the following code illustrates the use of <code>parfor</code> and Monte Carlo simulation to calculate an approximate value for \\( \\pi \\), using the formula (line #7):</p> \\[ \\pi \\approx \\lim_{N_{MC} \\to \\infty} {4 \\sum_{n=1}^{N_{MC}} I(x_i^2 + y_i^2 &lt; 1.0) \\over N_{MC}}; x_i, y_i \\sim \\mathcal{U}(0,1), \\] <p>where the symbol \\( \\mathcal{U}(0,1) \\) represents the random uniform distribution for the indicated interval and \\( I(boolean) \\) is an indicator function.</p> Matlab code for parfor demonstration<pre><code>N = 1e6;\nout = zeros(1, num_workers);\nparfor i = 1:num_workers\n    xy = rand(N,2);\n    out(i) = sum(sum(xy.^2,2)&lt;=1);\nend\nmypi = 4*sum(out)/(N*num_workers)\n</code></pre> <p>At the end of the parallel computations, the allocated parpool within Matlab can be released with the following command:</p> <pre><code>delete(gcp('nocreate'))\n</code></pre>"},{"location":"Applications/#running-matlab-with-gui-directly-on-a-compute-node","title":"Running Matlab with GUI directly on a compute node","text":"<p>First, get a compute node and launch vnc server from the node, for example:</p> <pre><code>srun -p k2-hipri -N 1 -n 6 --mem=10G --time=1:00:00 --pty bash\nvncserver\n</code></pre> <p>In this case, let us assume that the output of <code>vncserver</code> is</p> <p>New 'node117.pri.kelvin2.alces.network:1 (jsan)' desktop is node117.pri.kelvin2.alces.network:1</p> <p>Starting applications specified in /users/jsan/.vnc/xstartup Log file is /users/jsan/.vnc/node117.pri.kelvin2.alces.network:1.log</p> <p>Then, open a local terminal and launch a forward tunnel to the compute node by following these steps:</p> <ol> <li>Go to the directory which contains the kelvin key in your PC/laptop <pre><code>cd /drives/c/Users/jsan/.ssh\n</code></pre></li> <li>Create the tunnel (in this example illustrated with the command below, all input sent via port 5903 on your local host is being forwarded via port 5901 to the compute node \"node117.pri.kelvin2.alces.network\". If the <code>vncserver</code> output above were \"node117.pri.kelvin2.alces.network:7\", then the port number will be 5907 instead of 5901. Clearly, users must replace the username \"jsan\" and the key's filename by the corresponding information for their accounts) <pre><code>ssh -L 5903:node117.pri.kelvin2.alces.network:5901 -p 55890 -i ./kelvin-key jsan@login.kelvin.alces.network\n</code></pre></li> <li>Connect to the tunnel using your installed VNC application (the example shown in the figure below uses TurboVNC in Windows OS. More details or troubleshooting can be found in the VNC session) </li> <li>Once in the opened terminal for the connected compute node, launch the matlab application: <pre><code>module load matlab/R2022a\nmatlab\n</code></pre></li> </ol> <p>This time matlab GUI will be opened as shown in the figure below. For a better experience, use the \"full screen\" button in the VNC toolbar, and use the combination keys Ctrl+Alt+Shift + F to escape from the full screen mode. Note also that using the instruction <code>feature('numcores')</code> inside the Matlab's GUI session shows correctly the number of allocated CPU cores in the compute node (CPU cores equal to 6 in this example).</p> <p></p>"},{"location":"Applications/#running-matlab-with-the-use-of-gpus","title":"Running Matlab with the use of GPUs","text":"<p>First, get a GPU node and launch vnc server from the node, for example:</p> <pre><code>srun -p k2-gpu -N 1 -n 4 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nvncserver\n</code></pre> <p>Then, similarly as done in the previous section, (1) open a local terminal and launch a forward tunnel to the GPU node; (2) use a VNC application to connect to the tunnel; (3) load and launch the Matlab software.</p> <p>To test the use of the GPU device within Matlab code, copy and paste the script file <code>matlab_gpu_example.m</code> below.</p> matlab_gpu_example.m<pre><code>% The Mandelbrot algorithm iterates over a grid of real and imaginary parts.\n% The following code defines the number of iterations, grid size, and grid limits.\nmaxIterations = 500;\ngridSize = 1000;\nxlim = [-0.748766713922161, -0.748766707771757];\nylim = [ 0.123640844894862,  0.123640851045266];\n\n% Use the gpuArray function to transfer data to the GPU and create a gpuArray object.\nx = gpuArray.linspace(xlim(1), xlim(2), gridSize);\ny = gpuArray.linspace(ylim(1), ylim(2), gridSize);\n\n% Many Matlab functions support gpuArrays and run directly on the GPU (e.g., meshgrid)\n% The function \"ones\", below, create an array of ones directly on the GPU\n[xGrid,yGrid] = meshgrid(x,y);\nz0 = xGrid + 1i*yGrid;\ncount = ones(size(z0), 'gpuArray');\n\n% The code below implements the Mandelbrot algorithm, fully running on the GPU\nz = z0;\nfor n = 0:maxIterations\n    z = z.*z + z0;\n    inside = abs(z) &lt;= 2;\n    count = count + inside;\nend\ncount = log(count);\n\n% Plot the results\nimagesc(x, y, count);\ncolormap([jet(); flipud(jet()); 0 0 0]);\naxis off;\n</code></pre> <p>The following results should appear when you run this script in the GUI that is running on the cluster's GPU, as per below. As shown in the Matlab command window, the output of calling the function <code>whos</code> reveal the many variables (gpuArray objects) that are still allocated on the GPU.</p> <p></p>"},{"location":"Applications/#running-matlab-in-background-sbatch","title":"Running Matlab in background: sbatch","text":"<p>Finally, the most convenient way to use the cluster resources may be to perform Matlab calculations in background, using <code>sbatch</code> functionality instead of <code>srun</code>. This is critical, mainly for analysis where calculations may take several hours or days. To demonstrate this, simply copy and paste the following \"sbatch\" script, called <code>gpu_example.sh</code>, as an example which relies on the same script prepared above to calculate the Mandelbrot solution.</p> <p>gpu_example.sh<pre><code>#!/bin/bash\n\n#SBATCH --job-name=test\n#SBATCH -N 1\n#SBATCH -n 4\n#SBATCH --mem=20G\n#SBATCH --time=00:10:00\n#SBATCH --partition=k2-gpu\n#SBATCH --gres=gpu:1g.10gb:1\n#SBATCH --output=test_%j.log\n\nmodule load matlab/R2022a\n\n# Ulster University (UU) users must use UU's licence by declaring\n# (removing comments) these environmet variables:\n#export MLM_LICENSE_FILE=27000@193.61.190.229\n#export LM_LICENSE_FILE=27000@193.61.190.229\n\nmatlab -nosplash -nodisplay -r \"matlab_gpu_example; saveas(gcf, 'Mandelbrot'); exit;\"\n</code></pre> In the code above, line #19, Matlab is called in \"nosplash\" and \"nodisplay\" mode to execute the <code>matlab_gpu_example.m</code> script. Besides, note that the code will run in background, i.e., without display, therefore it must use some Matlab function like <code>saveas</code>, as shown in the code, which saves the graphical output. As a result, the Matlab file that must be listed now in the working directory, <code>Mandelbrot.fig</code>, should contain the visual results. Finally, to run this \"sbatch\" script, run the following command in your terminal.</p> <pre><code>sbatch gpu_example.sh\n</code></pre>"},{"location":"Applications/#anaconda","title":"Anaconda","text":"<p>Anaconda is a software that allows the users to manage environments to install local libraries and software, particularly Python and R programming languages, for scientific computing. Thus, greatly easing software management and reducing possible incompatibilities among different tools, libraries or software versions. Kelvin2 offers to users the following modules:</p> Anaconda modules<pre><code>apps/anaconda/2.5.0/bin\napps/anaconda3/2021.05/bin\napps/anaconda3/2022.10/bin\napps/anaconda3/5.2.0/bin\n</code></pre>"},{"location":"Applications/#python","title":"Python","text":"<p>Python is a high-level, multi-purpose programming language that support several paradigms such as structural, functional and object-oriented programming, together with garbage collection, in order to support code readability and fast development. With the provision of multiple especialized libraries, Python has become one of the most preferred languages for scientific programming, including machine learning and artificial intelligence, as well as bioinformatics among many other applications.</p> <p>Kelvin2 supports the following modules for Python users: Python modules<pre><code>apps/python/2.7.8/gcc-4.8.5\napps/python3/3.10.0/gcc-4.8.5\napps/python3/3.10.5/gcc-9.3.0\napps/python3/3.4.3/gcc-4.8.5\napps/python3/3.5.2/gcc-4.8.5\napps/python3/3.6.4/gcc-4.8.5\napps/python3/3.7.4/gcc-4.8.5\napps/python3/3.7.9/gcc-10.2.0\napps/python3/3.8.5/gcc-4.8.5\n</code></pre></p> <p>Notice that above are listed different Python's versions which in turn rely on different versions of the GCC compiler. The selected module must correspond to the tools or analysis pipeline that programmers have in mind, as some analysis and Python libraries may require older or newer versions of the Python/GCC software.</p> <p>The next code snippet illustrates simply how to load a module and install a package in the default location (gridware folder located in the users' home folder): <pre><code>module load apps/python3/3.10.5/gcc-9.3.0\npython3 -m pip install reportseff\nreportseff -u $USER\n</code></pre></p> <p>Note: On internet blogs/forums, users will often find the recommendation <code>pip install &lt;package name&gt;</code> to install a particular tool. In this case, for the same install in Kelvin2, it is recommended to precede the command with \"python3 -m\" because by default pip will refer to the Python 2.7 version, which is always available from command line in Kelvin2. That is, always use <code>python3 -m pip install &lt;package name&gt;</code> to install the package.</p> <p>In the previous example, reportseff is a very usefull tool to monitor the efficient utilization of cluster resources (time, RAM, CPU/GPU) that may be critical for your work. It should be used in addition to <code>sacct</code> command, because it helps to optimize your jobs which may translates on significantly lower queue times. For instance, the reportseff's outcome below shows some job statistics (greed/red colour codes highlight good/poor resources utilization) for selected jobs.</p> <p></p> <p>This approach, using directly Python modules and installing with pip (Python's PyPI), is mainly recommended for simpler or only-one-time analysis, or tests. Particularly, it has two main drawbacks which are discussed next together with recommendations:</p> <ol> <li>It uses the home folder quota (~50 GB hard disk + 100,000 files limit) To sort this out, users must declare/modify the environment variables \"PATH\" and \"PYTHONPATH\". For example, <pre><code>mkdir /mnt/scratch2/users/$USER/gridware\nexport PATH=/mnt/scratch2/users/$USER/gridware/bin/:$PATH\nexport PYTHONPATH=/mnt/scratch2/users/$USER/gridware/site-packages/:$PYTHONPATH\n</code></pre> creates and uses the folder \"gridware\" in users' scratch folder, so the pip install will be redirected to the scratch instead of home folder.</li> <li>Some Python's tools will need also to install libraries in the default locations, e.g., /user/local The issue is that users do not have write permission in Kelvin2 system paths. In this case, as well as in the case that users need to install their own version of Python or other supporting libraries, it is strongly recommended to use Anaconda environments as discussed in the next section.</li> </ol>"},{"location":"Applications/#python-with-anaconda","title":"Python with Anaconda","text":"<p>Similar as with the use of Python modules above, it is recommended that users redirect the default installation paths to their scratch folder to save the quota limited resources. For instance, this can be done by modifying the environment variables \"CONDA_PKGS_DIRS\" and \"CONDA_ENVS_PATH\":</p> <pre><code>mkdir /mnt/scratch2/users/$USER/conda\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\n</code></pre> <p>In the examples below, it is very critical to select the hardware correctly (CPU or GPU) before doing the installation. Therefore, before installing the needed tools, use <code>srun</code> as shown below to select the compute node with the convenient hardware, i.e., correspondingly in any of the CPU/GPU  partitions available in Kelvin2.</p> <p>For example, failing to select a compute node with GPU device to install the Python's package may cause that installed tools do not recognize the GPU device on runtime; therefore, your program may crash or, worse, run in the CPU cores by default giving a false sense of security. The second case is very critical because your jobs may last significantly longer to perform the same computations, but also they would waste GPU resources by running only on the CPU cores. At the same time, other users will be unable to use the allocated GPU device(s).</p>"},{"location":"Applications/#example-1-installing-pytorch-and-ray-tune","title":"Example 1: Installing Pytorch and Ray tune","text":"<p>In this example, Pytorch is installed together with Raytune. The latter is a tool used for deep learning models optimization as it helps with the evaluation and selection of model hyperparameters (e.g., number of layers, neurons per layer, selection between different transfer or optimization functions, etc.)</p> Pytorch-Raytune install<pre><code>srun -p k2-gpu -N 1 -n 1 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nmodule load libs/nvidia-cuda/11.7.0/bin\nconda create --name py39torchRayA100 python=3.9\nsource activate py39torchRayA100\n(py39torchRayA100) conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n(py39torchRayA100) python3\n(py39torchRayA100) conda install pytorch-lightning -c conda-forge\n(py39torchRayA100) conda install -c conda-forge \"ray-air\"\n(py39torchRayA100) conda deactivate\n</code></pre> <p>Line 1 above shows the use of GPU slice partitions: <code>--gres gpu:1g.10gb:1</code>. As currently there are available 28 slices in Kelvin2, this partition is much less busy than the other GPU partitions, and at the same time would allow the users to install software in the A100 GPU devices, which would guarantee backward compatibility with GPU devices.</p> <p>Lines 2-5 setup the environment variables, create an Anaconda environment named <code>py39torchRayA100</code> while installing Python version 3.9, and activate the environment. This Python version is strictly required here as current Raytune installation version may crash with newer Python versions than 3.10.</p> <p>The next lines 6-10 are needed to install the required tools, particularly Raytune is installed in line 9. Here, the package \"ray-air\" contains most of the provided functionality by Raytune (Data, Train, Tune, Serve, etc.).</p> <p>After installation, users must check that Pytorch can correctly utilize the GPU resources available. For example, users must see something similar to the following output when running the Pytorch's functions:</p> Testing that torch can see GPU resources<pre><code>source activate py39torchRayA100\n(py39torchRayA100) python3\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.cuda.is_available()\nTrue\n&gt;&gt;&gt; torch.cuda.current_device()\n0\n&gt;&gt;&gt; torch.cuda.device(0)\n&lt;torch.cuda.device object at 0x7fec6d471850&gt;\n&gt;&gt;&gt; torch.cuda.get_device_name(0)\n'NVIDIA A100-SXM4-80GB MIG 1g.10gb'\n&gt;&gt;&gt; exit()\n(py39torchRayA100) conda deactivate\n</code></pre>"},{"location":"Applications/#example-2-installing-tensorflow-gpu-and-raytune","title":"Example 2: Installing tensorflow-gpu and Raytune","text":"<p>Similarly, for installing tensorflow-gpu and Raytune, follow these instructions:</p> Tensorflow-Raytune install<pre><code>srun -p k2-gpu -N 1 -n 1 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nmodule load libs/nvidia-cuda/11.7.0/bin\nconda create --name tensorflowRayA100 python=3.9\nsource activate tensorflowRayA100\n(tensorflowRayA100) conda install -c anaconda tensorflow-gpu\n(tensorflowRayA100) conda install -c conda-forge \"ray-air\"\n(tensorflowRayA100) conda deactivate\n</code></pre> <p>As above, users also must test that tensorflow can correctly see the GPU resources. For that purpose, after installing the software, use the following commands:</p> Testing that tensorflow can see GPU resources<pre><code>source activate tensorflowRayA100\n(tensorflowRayA100) python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n(tensorflowRayA100) conda deactivate\n</code></pre>"},{"location":"Applications/#example-3-installing-bindsnet","title":"Example 3: Installing Bindsnet","text":"<p>In this example, users can overview how to install Bindsnet in Kelvin2:</p> Bindsnet install<pre><code>srun -p k2-gpu -N 1 -n 1 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nconda create --name bindsnet\nexport PATH=/mnt/scratch2/users/$USER/conda/envs/bindsnet/bin/:$PATH\nsource activate bindsnet\n(bindsnet) conda install python=3.10\n(bindsnet) python3 -m pip install git+https://github.com/BindsNET/bindsnet.git\n(bindsnet) python3 -m pip show bindsnet\n(bindsnet) python3 -m pip install pytest\n(bindsnet) conda deactivate\n</code></pre> <p>The apparent simplicity of above instructions hides some complexities. For example, the declaration of the \"PATH\" environment variable in line 4 may be neccesary when PyPi/pip is combined with Anaconda, as some of the installed packages may still be missing after installation, unexpectedly. Also, in line 6, Python's version 3.10 was installed as Bidsnet most recent version at this moment seems not to be compatible with older Python versions.</p> <p>Finally, when installing Bindsnet in line 7, it automatically finds and installs all needed dependencies, including the compatible versions for torch, torchvision, etc. Therefore, we do not recommend to install any package before bindsnet, otherwise bindsnet installation can fail. Then, in line 8, the command <code>python3 -m pip show bindsnet</code> allows to get the physical path where Bindsnet is installed, which may be necessary to access some folders with testing examples and scripts. For the same reasons, users may want to install the package \"pytest\", as it is done in line 9.</p> <p>As Bindsnet will install automatically Pytorch as one of its dependencies, we strongly recommend to test that Pytorch can see the GPU resources, as discussed in Example 1.</p> <p>The following is an example of sbatch script that uses the installed Bindsnet package, which can be launched from command line using the command <code>sbatch bindsnet_example.sh</code>. Of course, users must have first to prepare their python codes, with the main file name provided in line 19. </p> bindsnet_example.sh<pre><code>#!/bin/bash\n#SBATCH --job-name=nnet\n#SBATCH -N 1\n#SBATCH -n 4\n#SBATCH --mem=40G\n#SBATCH --partition=k2-gpu\n#SBATCH --gres gpu:v100:1\n##SBATCH --gres gpu:a100:1\n#SBATCH --time=03:00:00\n#SBATCH --output=nnet_%j.log\n\nmodule load apps/anaconda3/2021.05/bin\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\nexport PATH=/mnt/scratch2/users/$USER/conda/envs/bindsnet/bin/:$PATH\n\nsource activate /mnt/scratch2/users/$USER/conda/envs/bindsnet\n\npython3 &lt;code_file_name&gt;.py\n</code></pre> <p>VERY IMPORTANT: The three examples above were discussed to highlight the complexities of installing packages in Python and Anaconda, mainly when there may be issues related to package version incompatibilities, the proper Python's version (maybe an older or newer version is strictly required), package installation order, or the possible necessity to install supporting libraries.</p>"},{"location":"Applications/#jupyter-notebook","title":"Jupyter notebook","text":"<p>Here, we present simple instructions to install and use Jupyter notebook in Kelvin2, in the case for training deep learning networks with tensorflow-gpu.</p> Jupyter notebook installation<pre><code>srun -p k2-gpu -N 1 -n 4 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\n#srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nconda create -n tf-gpu tensorflow-gpu\n# export PATH=/mnt/scratch2/users/$USER/conda/envs/tf-gpu/bin/:$PATH\nsource activate tf-gpu\n(tf-gpu) conda install -c anaconda jupyter\n(tf-gpu) conda deactivate\n</code></pre> <p>If the user does not need to use GPU resources, for example, the plan is to run on CPU cores, then <code>srun</code> instruction in line 2 must be used, instead of line 1, to get an interactive session in CPU nodes for the installation. Also, ignore the installation of tensorflow part if the only purpose is to install Jupyter notebook to be used with other packages, such as Pandas or numpy. In that case, just create the environment in line 4 with the following instruction: <code>conda create -n &lt;environment_name&gt;</code>.</p> <p>Finally, to access Jupyter notebook remotely from the (local) browser in the user's PC/laptop, the user must launch a Jupyter server connection and create a tunnel to it. To lauch the server, run these commands:</p> <pre><code>srun -p k2-gpu -N 1 -n 4 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\n#srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nsource activate tf-gpu\n(tf-gpu) jupyter notebook --ip $(ip addr show em1 | grep 'inet ' | awk '{print $2}' | cut -d/ -f1) --no-browser\n</code></pre> <p>Jupyter notebook has to run in a compute node. That is why first need to allocate a compute node with <code>srun</code> command as shown in line 1 or 2 for a GPU or CPU node. Here, the server is lauched in line 5. Some critical output is printer, mainly the IP and port number, which should be annotated for using later to create the tunnel.</p> <p>For the last part, open a local terminal in the user's PC/laptop, and enter the following instructions:</p> <pre><code>cd /drives/c/Users/&lt;local_user&gt;/.ssh\nssh -p 55890 -i ./my-kelvin-key &lt;user_name&gt;@login.kelvin.alces.network -NL 8888:10.10.15.3:8888\n</code></pre> <p>In line 1, it is advisable to browse first to the folder where ssh Kelvin2 key is stored. This example uses Mobaxterm, for which drives locations start with \"/drives/...\". Line 2 stablishes the tunnel using <code>ssh</code> command, where it has been assumed that the IP and port number annotated above are \"10.10.15.3\" and \"8888\", respectively.</p> <p>If not errors are reported during the execution of these commands, and the terminal looks like hanging out, then everything is ok, and the last step is to open a local browser and enter the adress \"http://127.0.0.1 ...\", which must have been shown in the output when the server connection was created.</p>"},{"location":"Applications/#r","title":"R","text":"<p>R is an open-source and free software which provides a programming language designed purposedly for statistical analyses. It is also highly preferable for data mining tasks and machine learning analysis. In Kelvin2 are available the following R's modules:</p> <pre><code>apps/R/3.2.1/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.2.5/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.3.2/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.4.2/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.5.1/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.6.1/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/3.6.3/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/4.0.4/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/4.1.0/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/4.1.2/gcc-4.8.5+lapack-3.5.0+blas-3.6.0\napps/R/4.1.3/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\napps/R/4.2.2/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\napps/R/4.3.0/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\n</code></pre> <p>By default, installing packages in R will fill up the users' quota (~50 GB hard disk + 100,000 files limit); therefore, it is advisable that the users setup an installation path for the packages. Below, the default install location is redirected to the user's scratch folder. The most transparent way to perform this operation is to create the file \".Renviron\" in the home folder, which will keep the necessary R's environment variables to setup correctly the settings.</p> Setup R environment<pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmkdir /mnt/scratch2/users/$USER/R\nmkdir /mnt/scratch2/users/$USER/R/lib\ncd ~\necho \"R_LIBS_USER=/mnt/scratch2/users/$USER/R/lib\" &gt; .Renviron\necho \"R_LIBS=/mnt/scratch2/users/$USER/R/lib\" &gt;&gt; .Renviron\necho \"PKG_CONFIG_PATH=/mnt/scratch2/$USER/R/lib/pkgconfig\" &gt;&gt; .Renviron\necho \"\" &gt;&gt; .Renviron\nmodule load apps/R/4.3.0/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\nR\n&gt; .libPaths()\n&gt; quit()\n</code></pre> <p>In line 1, a compute node is requested to perform packages installation and work with R. Do not use login nodes to perform these operations. Lines 2-3 create the needed R's library folder. If they are not created, environment initialization may fail to be setup correctly. This operation needs to be done only one time at the very beginning of your R utilization in Kelvin2. It is neccesary to create \".Renviron\" in the home folder which is guaranteed in line 4. Then, the \".Renviron\" file is prepared in line 5-8. Notice, in line 8, this file must end with a newline character. If not, the instruction in the last line will be ignored without warning nor error. The environment file is also setup once, or every time the user wish to change the default install and library folders. Once the R's module is loaded and R is lauched in lines 9-10, the test of running the R's function <code>.libPaths()</code> must show correctly the new library path (line 11).</p>"},{"location":"Applications/#installing-rs-packages-using-an-available-module","title":"Installing R's packages using an available module","text":"<p>After setting up correctly the \".Renviron\" file as presented above, installation of R's packages can proceed inside the R's application as follows:</p> <pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load libpng/16\nmodule load apps/R/4.3.0/gcc-9.3.0+lapack-3.9.0+blas-3.8.0\nexport HDF5_USE_FILE_LOCKING='FALSE'\nR\n&gt; if (!requireNamespace('BiocManager', quietly = TRUE))\n&gt; install.packages('BiocManager')\n&gt; BiocManager::install('DESeq2')\n&gt; installed.packages()\n&gt; quit()\n</code></pre> <p>Here, some issues may arise, such as that packages will not be installed correctly if some dependencies, libraries, or settings are not guaranteed. Therefore, the commands in lines 2, 4 are necessary in the present example. After that, the installation of the packages \"BiocManager\" and \"DESeq2\" is most straightforwardly done as shown in lines 7-8. Finally, the call to <code>installed.packages()</code> in line 9 will show all the installed R's packages.</p> <p>When required libraries are not available in Kelvin2 module system, the users must contact the RSE Kelvin2 team. Otherwise, they can proceed to create an Anaconda environment, where installation of the R's preferred version, libraries and packages can be performed, as discussed in the next section.</p>"},{"location":"Applications/#r-with-anaconda","title":"R with Anaconda","text":"<p>To create an environment on the user's scratch folder to install a particular version of R, follow these instructions:</p> R installation in an Anaconda environment<pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\nconda create -n R412env -c conda-forge r-base=4.1.2\n</code></pre> <p>Here, the preferred version (4.1.2) is specified in line 5 with the argument \"r-base=4.1.2\". For other versions, check the Anaconda R's online documentation, for example: search https://anaconda.org/conda-forge/r-base for the most recent version or https://docs.anaconda.com/free/anaconda/packages/using-r-language/ for more information.</p>"},{"location":"Applications/#installing-packages-within-an-anaconda-environment","title":"Installing packages within an Anaconda environment","text":"<p>The following example illustrates how to use this environment for installing a particular R's package (HOMER: http://homer.ucsd.edu/homer/introduction/install.html), proceeding from a clean/new Kelvin2 terminal connection.</p> <pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2021.05/bin\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\nsource activate R412env\n(R412env)$ conda config --add channels defaults\n(R412env)$ conda config --add channels bioconda\n(R412env)$ conda config --add channels conda-forge\n(R412env)$ conda install -c bioconda samtools r-essentials bioconductor-deseq2 bioconductor-edger\n(R412env)$ conda install homer\n(R412env)$ conda update homer\n(R412env) conda deactivate\n</code></pre> <p>Users should notice that when working with R inside an Anaconda environment, the libraries must be installed with the command <code>conda install ...</code>. Whereas, the packages can be installed either in the same way by using conda install, or from the R's command line; for instance, using the function <code>installed.packages()</code>, as shown in a section above. </p>"},{"location":"Applications/#ansys","title":"Ansys","text":"<p>For more than 50 years, Ansys software has enabled innovators across industries to push boundaries with the predictive power of simulation. From sustainable transportation and advanced semiconductors, to satellite systems and life-saving medical devices, the next great leaps in human advancement will be powered by Ansys. https://www.ansys.com</p> <p>Ansys is a licensed software. In order to use it, users have to be registered in the license server. If you are not already included in the license server, you must contact the person in charge of it:</p> <p>QUB: TBA  Ulster: TBA</p> <p>When the module is loaded, license parameters for QUB are loaded by default. Ulster users should change these license parameters in their batch script or interactive session. Contact the person in charge of the license for details. User manual for Ansys is not publicly available, and only licensed users can access to it. To access to this material, register yourself in the Ansys web seite.  https://customercenter.ansys.com</p>"},{"location":"Applications/#load-ansys-module","title":"Load Ansys module","text":"<p>The latest installed version on Kelvin-2 is the 2023R1. To load Ansys:</p> <pre><code>$ module load apps/ansys/2023.1\n</code></pre>"},{"location":"Applications/#ansys-fluent-batch-script-example","title":"Ansys Fluent batch script example","text":"<pre><code>#!/bin/bash\n\n#SBATCH --job-name=myfluentjob\n#SBATCH --output=myoutput.out\n#SBATCH --error=myerror.err\n#SBATCH --nodes=1\n#SBATCH --ntasks=32\n#SBATCH --partition=k2-hipri\n#SBATCH --mem=100G\n\nmodule load apps/ansys/2023.1\n\n## QUB's license, already loaded with the module\n#export ANSYSLI_SERVERS=2325@143.117.212.118\n#export ANSYSLMD_LICENSE_FILE=1055@143.117.212.118\n\n## Ulster's license\n#export ANSYSLI_SERVERS=2325@193.61.145.219\n#export ANSYSLMD_LICENSE_FILE=1055@193.61.145.219\n\n# Set architecture of the CPU (in this case amd64) and environment variables\nexport FLUENT_ARCH=lnamd64\nexport FL_TMPDIR=$SCRATCH/tmp\n\n# Create our hosts file \nsrun hostname -s | sort &gt; hosts.$SLURM_JOB_ID.txt\n\n#Run Ansys Fluent.\nfluent 3ddp -g -t$SLURM_NTASKS -pinfiniband -mpi=openmpi -cnf=hosts.$SLURM_JOB_ID.txt -i my_fluent_input &gt; my_fluent_output.res\n</code></pre>"},{"location":"Applications/#paraview","title":"Paraview","text":"<p>A recommended method to run Paraview on Kelvin-2, it is to start a PV server on Kelvin-2, and to use your local machine to visualize the graphical interface. In this case, you need an installation of Paraview in your local machine as well.</p> <p>With the following method, Paraview can be run in parallel, and use the local machine to work with the graphical interface. It is necessary to set a tunnel through the Kelvin-2 login node.</p> <p>As example, we are going to explain how to set the tunnel and connect with a computer outside the QUB campus. If the local machine is insithe the QUB campus, you have to remove the specification of the port <code>-p</code>, the identification private-key file <code>-i</code>, and to change the IP name to <code>kelvin2.qub.ac.uk</code>.</p> <p>Step 1. Run pvserver in a computing node of Kelvin-2.</p> <p>Open the session in Kelvin-2 as usual, and then open an interactive session. <code>&lt;N&gt;</code> will be the number of cores you require for your parallel job on Paraview.</p> <pre><code>$ srun --pty --partition=k2-hipri --ntasks=&lt;N&gt; --mem-per-cpu=2G /bin/bash\n</code></pre> <p>Then, you are transferred to a computing node.</p> <p>Now load the necessary modules, for paraview, and for OpenMPI.</p> <pre><code>[&lt;uid&gt;@nodeNNN [kelvin2] ~]$ module load apps/paraview/5.8.1/bin\n[&lt;uid&gt;@nodeNNN [kelvin2] ~]$ module load mpi/openmpi/4.0.4/gcc-9.3.0+ucx-1.8.0\n</code></pre> <p>Run the server with the command</p> <pre><code>[&lt;uid&gt;@nodeNNN [kelvin2] ~]$ mpirun -np &lt;N&gt; pvserver --force-offscreen-rendering --server-port=&lt;YYYY&gt;\n</code></pre> <p>Where <code>&lt;uid&gt;</code> is your Kelvin-2 username, <code>&lt;N&gt;</code> is the number of MPI tasks you want to run Paraview, and <code>&lt;YYYY&gt;</code> is a port number your choice.</p> <p>Step 2. Create the tunnel to the port <code>&lt;YYYY&gt;</code>, through the port <code>&lt;XXXX&gt;</code> of your local machine.</p> <p>In a different shell on your local machine, open the tunnel with the command:</p> <pre><code>(local)$ ssh -L &lt;XXXX&gt;:nodeNNN:&lt;YYYY&gt; -p 55890 -i ~/.ssh/my-kelvin-key &lt;uid&gt;@login.kelvin.alces.network\n</code></pre> <p>Where <code>&lt;uid&gt;</code> is your Kelvin-2 username, <code>&lt;YYYY&gt;</code> is the port number you let open in the former step, and <code>&lt;XXXX&gt;</code> is a different port number your choice on your local machine. <code>nodeNNN</code> is the computing node that the <code>srun</code> command directed you in the former step.</p> <p>Step 3. Open paraview in your local machine.</p> <p>Inside the application click: File - Connect - Give a name to the session, e. g. \"remote machine\" - In \"Server Type\" choose: Client / Server - host: localhost - Port: <code>&lt;XXXX&gt;</code>  (from step 2)</p> <p>Configure - Startup Type: Manual - Press Save</p> <p>Connect Once you created the client the first time, you can recover it the next time you open Paraview, you do not need to create it again, and you can connect directly in the step 3.</p> <p></p>"},{"location":"Applications/#containers-singularity","title":"Containers. Singularity.","text":"<p>In Kelvin-2, we use singularity to run containers. It has as main advantage, that it does not require root privileges to install the containers. Because of that, it is the mostly used container in HPC systems.</p> <p>To activate singularity, one of the modules should be loaded</p> <pre><code>apps/singularity/3.10.0\napps/singularity/3.4.2\n</code></pre> <p>More detail about using singularity and containers on Kelvin-2 can be found in the  online seminar</p>"},{"location":"Applications/#running-a-docker-image-on-singularity","title":"Running a Docker image on Singularity","text":"<p>You need the docker image file.  <code>myimage.img</code> As a tarball  <code>mytarball.tar</code></p> <p>Go to a compute node</p> <pre><code>$ srun --pty --partition=k2-hipri --ntasks=1 --mem-per-cpu=2G bash\n</code></pre> <p>Load the module</p> <pre><code>$ module load apps/singularity/3.4.2\n</code></pre> <p>Convert the tarball to singularity: Firstly go to the directory where to tarball is located</p> <pre><code>$ cd /path/to/tarball\n</code></pre> <p>Convert the tarball:</p> <pre><code>$ singularity build --sandbox mytarball docker-archive://mytarball.tar\n</code></pre> <p>Execute the image</p> <pre><code>$ singularity shell myimage\n$ singularity exec myimage mycommand\n$ singularity run myimage\n</code></pre>"},{"location":"Applications/#create-the-tarball-from-a-docker-image","title":"Create the tarball from a Docker image","text":"<p>These steps should be done in your local computer, where you have docker. Once you create the tarball, you have to copy it to Kelvin-2 and create the singularity image in Kelvin-2.</p> <pre><code>&lt;my_local_machine&gt;$ docker save my_docker_image -o mytarball.tar\n</code></pre>"},{"location":"Compilers/","title":"Compilers","text":"<p>Kelvin-2 has a large set of compilers and libraries for those users who compile their own self-programmed applications. We hardly recommend avoiding the system compilers, and to use those ones which are installed as modules. Modules for compilers as flagged in general as</p> <pre><code>compilers/&lt;name&gt;/&lt;version&gt;\n</code></pre> <p>And libraries</p> <pre><code>libs/&lt;name&gt;/&lt;version&gt;/&lt;compiler&gt;\n</code></pre> <p>where  states the compiler and version that it was compiled with, and in some cases, other libraries as dependencies. If you are going to use a precompiled library, be sure to use for your application the same compiler and version that the particular library was compiled with."},{"location":"Compilers/#compilers-available-on-kelvin-2","title":"Compilers available on Kelvin-2","text":"<p>On Kelvin-2, for usual programming languages as C, C++, or Fortran, we recommend using the GNU compiling suite. It is well tested, and it is currently the fastest for AMD systems. It is also a universal compiling suite, so any code will compile with it.</p> <pre><code>compilers/gcc/10.2.0\ncompilers/gcc/10.3.0\ncompilers/gcc/5.1.0\ncompilers/gcc/6.4.0\ncompilers/gcc/7.2.0\ncompilers/gcc/9.3.0\n</code></pre> <p>Other compilers installed in Kelvin-2 are the AOCC, Clang (as part of library llvm), or Nvidia nvhp (only in GPU nodes).</p> <pre><code>aoc-compiler/2.2.0\naoc-compiler/3.0.0\nllvm/12.0.0/gcc-9.3.0\nnvhpc/22.7\n</code></pre> <p>In consistence with the general practice, jobs must not be run into the login nodes, and that includes compilation.  The compilers check the hardware of the node where they are working, and they produce an executable adapted to the architecture of the node.  Login nodes have different architecture than compute nodes, so an application compiled in the login nodes will likely not work in the compute nodes.</p> <p>To compile, an interactive session should be opened in a compute node.  All the compute nodes in Kelvin-2 have the same architecture, so in general it does not matter in which one of them the compilation is performed.  High-memory nodes have more RAM memory, but the model of the RAM and the processors are the same as the standard nodes,  so a program compiled in the standard nodes will work in the high-memory nodes.</p> <p>One exception is to compile a program designed to work on the GPUs.  In this case, the session must be allocated in the k2-gpu partition, and it must be allocated a GPU resource where the application will be executed.  For compatibility, we recommend using the A100 GPUs to compile. Further details about how to compile for GPUs will be stated in the specific section.</p> <pre><code>srun --pty --partition=k2-gpu --ntasks=1 --mem-per-cpu=4G --gres gpu:a100:1 bash\n</code></pre>"},{"location":"Compilers/#example-of-compilation","title":"Example of compilation","text":"<p>hello_world.c</p> <pre><code>// Program to print Hello World using C language\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid main(void)\n{\n    printf(\"Hello World... \\n\");\n}\n</code></pre> <p>Compile and execute</p> <pre><code>[&lt;user&gt;@login1 [kelvin2] ~]$ srun --pty --partition=k2-hipri --ntasks=1 --mem-per-cpu=100M bash\n[&lt;user&gt;@node162 [kelvin2] ~]$ module load compilers/gcc/9.3.0\n[&lt;user&gt;@node162 [kelvin2] ~]$ gcc -O2 -o hello_world.x hello_world.c\n[&lt;user&gt;@node162 [kelvin2] ~]$ ./hello_world.x\nHello World...\n</code></pre>"},{"location":"Compilers/#compiling-parallel-applications","title":"Compiling parallel applications","text":"<ul> <li>OpenMP</li> </ul> <p>All the compilers have integrated the libraries to execute in parallel using the Open Multi Processor protocol.  In the case of the GNU compiler, the OpenMP protocol is activated just adding the flag</p> <pre><code> -fopenmp\n</code></pre> <p>to the compilation command.</p> <p>hello_world_omp.c</p> <pre><code>// OpenMP program to print Hello World\n// using C language\n\n// OpenMP header\n#include &lt;omp.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(int argc, char* argv[])\n{\n\n    // Beginning of parallel region\n    #pragma omp parallel\n    {\n\n        printf(\"Hello World... from thread = %d\\n\",\n               omp_get_thread_num());\n    }\n    // Ending of parallel region\n}\n</code></pre> <p>Compile and execute</p> <pre><code>[&lt;user&gt;@login1 [kelvin2] ~]$ srun --pty --partition=k2-hipri --ntasks=8 --mem-per-cpu=100M bash\n[&lt;user&gt;@node162 [kelvin2] ~]$ module load compilers/gcc/9.3.0\n[&lt;user&gt;@node162 [kelvin2] ~]$ gcc -O2 -fopenmp -o hello_world_omp.x hello_world_omp.c\n[&lt;user&gt;@node162 [kelvin2] ~]$ ./hello_world_omp.x\nHello World... from thread = 3\nHello World... from thread = 2\nHello World... from thread = 6\nHello World... from thread = 7\nHello World... from thread = 4\nHello World... from thread = 5\nHello World... from thread = 0\nHello World... from thread = 1\n</code></pre> <ul> <li>MPI</li> </ul> <p>To compile using the Message Passing Interface protocol, in Kelvin-2 it is necessary to load the modules for those libraries and change the compiling commands.  The modules that activate the MPI libraries are flagged in Kelvin-2 as</p> <pre><code> mpi/&lt;name&gt;/&lt;version&gt;/&lt;compiler&gt;\n</code></pre> <p>where  points the compiler and version that the MPI libraries were compiled with.  This is important for compatibility of the compilations, it should be used the same compiler for the application than the one that was used to compile the MPI libraries. <p>The available MPI implementations on Kelvin-2 are</p> <pre><code> mpi/mpich/3.0.4/gcc-4.8.5\n mpi/mpich/4.1.1/gcc-10.3.0\n mpi/mpich2/1.5/gcc-4.8.5\n mpi/mvapich/1.2.0-3635/gcc-4.8.5\n mpi/mvapich2/1.6/gcc-4.8.5\n mpi/openmpi/1.10.1/gcc-4.8.5\n mpi/openmpi/1.10.2/gcc-4.8.5\n mpi/openmpi/1.10.7/gcc-4.8.5\n mpi/openmpi/3.1.3/gcc-4.8.5\n mpi/openmpi/3.1.4/gcc-4.8.5\n mpi/openmpi/4.0.0/gcc-4.8.5\n mpi/openmpi/4.0.0/gcc-4.8.5+ucx-1.4.0\n mpi/openmpi/4.0.0/gcc-5.1.0\n mpi/openmpi/4.0.4/gcc-9.3.0+ucx-1.8.0\n mpi/openmpi/4.1.1/gcc-9.3.0\n</code></pre> <p>We hardly recommend using the OpenMPI compiling suite, it has been widely tested, and it works stable on Kelvin-2.  The MPICh suite has not been so deeply tested, and it is not guaranteed that applications compiled with it will work on Kelvin-2.  If you decide to use the MPICh suite, be sure to test your application before using it for production.</p> <p>To compile using MPI, the compiling commands should be changed.  For example, to compile a C program with the compiler GNU 9.3.0, the command to be used is</p> <pre><code> gcc &lt;flags&gt; -o my_executable.x my_C_program.c\n</code></pre> <p>And if we want to compile it in parallel using the MPI version 4.1.1, the compiling command should be changed to</p> <pre><code> mpicc &lt;flags&gt; -o my_executable.x my_MPI-C_program.c\n</code></pre> <p>In general, the compiling commands should be changed</p> <pre><code> gcc -&gt; mpicc\n g++ -&gt; mpicxx\n gfortran -&gt; mpifortran\n</code></pre> <p>These values are taken by default, and the command \"mpicc\" will use the GNU compiler and the MPI libraries.  If you want to use a different compiler, you have to specify it in the environment variables</p> <pre><code>OMPI_MPICC\nOMPI_MPICXX\nOMPI_FC\n</code></pre> <p>For example, to compile using the MPI library v4.1.1 with the clang compiler included in the module llvm v12.0.0, the environment variables should be defined as</p> <pre><code>export OMPI_MPICC=clang\nexport OMPI_MPICXX=clang++\nexport OMPI_FC=flang\n</code></pre> <p>and compile using the commands \"mpicc\", \"mpicxx\", \"mpifortran\".</p> <p>hello_world_mpi.c</p> <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    // Initialize the MPI environment\n    MPI_Init(NULL, NULL);\n\n    // Get the number of processes\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    // Get the rank of the process\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);\n\n    // Get the name of the processor\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name(processor_name, &amp;name_len);\n\n    // Print off a hello world message\n    printf(\"Hello world from processor %s, rank %d out of %d processors\\n\",\n           processor_name, world_rank, world_size);\n\n    // Finalize the MPI environment.\n    MPI_Finalize();\n}\n</code></pre> <p>Compile and execute</p> <pre><code>[&lt;user&gt;@login1 [kelvin2] ~]$ srun --pty --partition=k2-hipri --ntasks=8 --mem-per-cpu=100M bash\n[&lt;user&gt;@node162 [kelvin2] ~]$ module load compilers/gcc/9.3.0\n[&lt;user&gt;@node162 [kelvin2] ~]$ module load mpi/openmpi/4.0.4/gcc-9.3.0+ucx-1.8.0\n[&lt;user&gt;@node162 [kelvin2] ~]$ mpicc -O2 -o hello_world_mpi.x hello_world_mpi.c\n[&lt;user&gt;@node162 [kelvin2] ~]$ mpirun -np 8 ./hello_world_mpi.x\nHello world from processor node162.pri.kelvin2.alces.network, rank 0 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 4 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 5 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 1 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 3 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 6 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 7 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 2 out of 8 processors\n</code></pre> <ul> <li>OpenACC</li> </ul> <p>Open accelerator is a feature included in the Nvidia compiler.  It can be activated adding to the compilation sequence the flag</p> <pre><code> -acc\n</code></pre> <p>This option implies an intelligent check-up of the hardware at the time of the execution.  The compiler will use the available hardware resources at that time to optimize the acceleration of the marked regions of the code as \"OpenACC\",  multiple-CPUs or GPUs will be allocated by the compiler for the execution, the user does not need to specify the resources at the time of the execution.</p> <p>OpenACC is a feature of the Nvidia compiler. On Kelvin-2, it will work only on the GPU nodes.  If you want the compiler to take advantage of the GPU accelerators, you must allocate a GPU resource when you compile the code, preferably an A100 GPU.</p> <p>More information about OpenACC, including pdf user guides can be found in the web site  https://www.openacc.org</p> <p>hello_world_openacc.c</p> <pre><code>#include &lt;stdio.h&gt;\n#ifdef _OPENACC\n#include &lt;openacc.h&gt;\n#endif\n\nint main(void) {\n#ifdef _OPENACC\n    acc_device_t devtype;\n#endif\n\n    printf(\"Hello world from OpenACC\\n\");\n#ifdef _OPENACC\n    devtype = acc_get_device_type();\n    printf(\"Number of available OpenACC devices: %d\\n\", acc_get_num_devices(devtype));\n    printf(\"Type of available OpenACC devices: %d\\n\", devtype);\n#else\n    printf(\"Code compiled without OpenACC\\n\");\n#endif\n\n    return 0;\n}\n</code></pre> <p>Compile and execute</p> <pre><code>[&lt;user&gt;@login1 [kelvin2] ~]$ srun --pty --partition=k2-gpu --ntasks=1 --mem-per-cpu=1G --gres gpu:a100:1 bash\n[&lt;user&gt;@gpu111 [kelvin2] ~]$ module load nvhpc/22.7\n[&lt;user&gt;@gpu111 [kelvin2] ~]$ nvc -O2 -acc -o hello_world_openacc.x hello_world_openacc.c\n[&lt;user&gt;@gpu111 [kelvin2] ~]$ ./hello_world_openacc.x\nHello world from OpenACC\nNumber of available OpenACC devices: 1\nType of available OpenACC devices: 4\n</code></pre>"},{"location":"Compilers/#compiling-applications-that-use-gpus","title":"Compiling applications that use GPUs","text":"<p>To compile a program designed to work on the Graphical Processing Units of Kelvin-2, it is essential that it is compiled in a GPU node, so the queue \"k2-gpu\" must be allocated. For backwards compatibility, the program must be compiled in the latest model of GPU present in the machine, in our case, the Nvidia A100 GPUs. So, when allocating the interactive session to carry out the compilation, the resource A100 should be allocated with the flag</p> <pre><code>--gres gpu:a100:1\n</code></pre> <p>During the last years, the popularity of the specific GPU-focused programming languages, such as CUDA, has decreased. This is due to the publicly-available libraries have become more and more complete, and practically any mathematical operation that can benefit of the acceleration advantages of a GPU is present on those libraries. Some examples are \"cublas\" for linear-algebra operations, and \"cufft\" for Fourier transforms. Most of the libraries designed to work in the GPUs of Kelvin-2 can be found in the module for the Nvidia CUDA drivers:</p> <pre><code>libs/nvidia-cuda/11.0.3/bin\nlibs/nvidia-cuda/11.7.0/bin\n</code></pre> <p>These libraries can be including in the executables, as usual adding the flag \"-l\" to the compilation command, for example</p> <pre><code>gcc &lt;flags&gt; -lcublas -lcufft -o my_executable.x my_GPU_program.c\n</code></pre> <p>Nevertheless, if you require a very specific operation, that is not present in the available libraries, and it is necessary to use CUDA, the Nvidia compiler can be used. Currently, the Nvidia compiler is the only one installed on Kelvin-2 that can compile CUDA code</p> <pre><code>nvhpc/22.7\nnvhpc/22.7-byo\nnvhpc/22.7-nompi\n</code></pre> <p>This compiler can recognise the sections of the code in an intelligent way, so there is no need of more flags to point that it includes CUDA routines. To compile with Nvidia compiler, just use its usual commands</p> <pre><code>nvc &lt;flags&gt; -o my_executable.x my_CUDA_C_program.c\nnvc++ &lt;flags&gt; -o my_executable.x my_CUDA_C++_program.cxx\nnvfortran &lt;flags&gt; -o my_executable.x my_CUDA_Fortran_program.for\n</code></pre>"},{"location":"Connecting%20to%20Kelvin2/","title":"Connecting to Kelvin2","text":""},{"location":"Connecting%20to%20Kelvin2/#applying-for-a-kelvin2-account","title":"Applying for a Kelvin2 account","text":"<p>Complete the application form on our website to apply for an account on Kelvin2. Please allow 48 hours for your application to be processed. If your request is successful, you will receive confirmation via email along with your account credentials.</p>"},{"location":"Connecting%20to%20Kelvin2/#connecting-to-kelvin2-using-the-terminal","title":"Connecting to Kelvin2 using the terminal","text":"<p>Connecting to Kelvin2 is done via Secure Shell Protocol (SSH) using either the terminal (command prompt) that comes preinstalled with your operating system or a separate SSH client which offers additional features, such as PuTTY or MobaXterm. This section shows you how to connect to Kelvin2 using the terminal.</p> <p>Note</p> <p>Older versions of Windows, pre Windows 10 (Autumn 2018), do not have OpenSSH installed as standard for use in their Command Prompt or PowerShell. In this case a separate SSH client such as PuTTY or MobaXterm is recommended.</p> <p>There are two ways to connect to Kelvin2 depending on whether you are inside or outside the QUB network.</p>"},{"location":"Connecting%20to%20Kelvin2/#access-from-inside-the-qub-network","title":"Access from inside the QUB network","text":"<p>If you are a QUB user and you are connecting from inside the QUB network (either from being on premises or using VPN/Remote Desktop):</p> <ul> <li>Enter the following command into your terminal. <code>&lt;username&gt;</code> is your QUB staff or student number:   <pre><code>ssh &lt;username&gt;@kelvin2.qub.ac.uk\n</code></pre></li> <li>Enter your password when requested. This is the same password associated with your QUB Active Directory.</li> <li>If you have enabled multi-factor authentication (MFA), enter the verification code provided by your authenticator app when requested.</li> </ul> <p>Warning</p> <p>It is now mandatory to have MFA enabled on your Kelvin2 account. To set up MFA, follow the instructions provided further down this page.</p>"},{"location":"Connecting%20to%20Kelvin2/#access-from-outside-the-qub-network","title":"Access from outside the QUB network","text":"<p>If you are outside the QUB network you can only connect to Kelvin2 using an SSH key pair. This steps required are as follows:</p> <ol> <li>Generate an SSH key pair (and passphrase) on your machine</li> <li>Copy the public SSH key to Kelvin2.</li> <li>Connect to Kelvin2 using the SSH key and passphrase</li> </ol> <p>Steps 1 and 2 will only need to be completed once.</p> <p>1. Generate an SSH Key Pair</p> <p>The procedure for generating an SSH key pair on your terminal varies depending on your operating system. Expand the relevant option below:</p> Mac/Linux <p>Create the SSH key pair on your machine by entering the following command into your terminal: <pre><code>ssh-keygen -t rsa -f ~/.ssh/my-kelvin-key\n</code></pre> All users MUST set a passphrase when prompted.</p> <p>This will create the SSH key pair in your .ssh directory. This key pair consists of a private key <code>my-kelvin-key</code> and a public key <code>my-kelvin-key.pub</code></p> <p>It is the contents of the public key that you will want to transfer to Kelvin2. To display the contents of this file in the terminal, enter the following command: <pre><code>cat ~/.ssh/my-kelvin-key.pub\n</code></pre></p> <p>The private key should be kept safe and should not be shared with anybody.</p> <p>Please see our video on how to set up remote access via Mac.</p> Windows 10 (Autumn 2018) or later <p>Create the SSH key pair on your machine by entering the following command into your terminal: <pre><code>ssh-keygen\n</code></pre> Call the key <code>my-kelvin-key</code> and optionally specify the full path that you want to save it to. The default location, if none is specified, is your home directory, e.g. <code>C:\\Users\\&lt;username&gt;\\my-kelvin-key</code>.</p> <p>All users MUST set a passphrase when prompted.</p> <p>This will create the SSH key pair in the directory you have specified. This key pair consists of a private key <code>my-kelvin-key</code> and a public key <code>my-kelvin-key.pub</code></p> <p>It is the contents of the public key that you will want to transfer to Kelvin2. To display the contents of this file in the terminal, enter the following command: </p> <p><pre><code>type my-kelvin-key.pub\n</code></pre> The private key should be kept safe and should not be shared with anybody.</p> Older versions of Windows <p>Older versions of Windows do not have a built in SSH client and you will have to install one separately. Two popular choices are  PuTTY  and MobaXterm which have graphical tools to generate SSH keys called PuTTYgen and MobaKeyGen, respectively.</p> <p>2. Copy the Public SSH Key to Kelvin2</p> <p>If you are able to connect to Kelvin2 from inside the QUB network, but want to access from outside the network in future, then you are able to add the public key yourself. If you are outside the QUB network and therefore do not have access to Kelvin2,  send your public key to us, and an administrator will add it for you.</p> <p>To add your public key from inside the QUB network:</p> <ul> <li> <p>Log into Kelvin2 using your QUB Active Directory credentials.</p> </li> <li> <p>Type the following command into your terminal on Kelvin2 to open the <code>authorized_keys</code> file using the vim text editor.    <pre><code>vi ~/.ssh/authorized_keys\n</code></pre></p> </li> <li>Once vim has opened, navigate to the end of the file and access \"Insert Mode\" by pressing Shift+G then Shift+A. The public SSH key already listed in the <code>authorized_keys</code> file labeled 'Alces Clusterware HPC Cluster Key' should not be modified or deleted.</li> <li>Press 'Enter' to create a new line</li> <li>On your local computer, copy the contents of <code>my-kelvin-key.pub</code> to the clipboard</li> <li>Paste this to the end of your <code>authorized_keys</code> file in Kelvin2 by right-clicking in the vim editor</li> <li>Press Esc to re-enter \"Normal Mode\"</li> <li>Save the file and exit the vim editor by typing the following command and press 'Enter'.   <pre><code>:wq\n</code></pre></li> </ul> <p>3. Connect to Kelvin2 using the SSH Key</p> <p>After this initial set-up process is complete, you can connect to Kelvin2 as follows:</p> <ul> <li>Enter the following command into your terminal. For QUB users, <code>&lt;username&gt;</code> will be your staff or student number. For UU/EPSRC users, <code>&lt;username&gt;</code> will typically be your first initial and surname. <pre><code>ssh -p 55890 -i /path/to/ssh/private/key &lt;username&gt;@login.kelvin.alces.network\n</code></pre></li> <li>Enter your passphrase when requested. This is the passphrase that you set up when creating the SSH key pair.</li> <li>Enter the verification code provided by your authenticator app when requested. If you have not yet set up MFA, follow the instructions provided in the following section</li> </ul>"},{"location":"Connecting%20to%20Kelvin2/#multi-factor-authentication-mfa-on-kelvin2","title":"Multi-Factor Authentication (MFA) on Kelvin2","text":"<p>To enhance the security of your Kelvin2 account, you are now requested to enable Multi-Factor Authentication (MFA). This involves a one-time activation process and then entering the verification code provided by your authenticator application on your mobile device (e.g. Microsoft Authenticator) every time you connect.</p> <p>For a step by step guide on how to set up MFA, Please see our set up video.</p>"},{"location":"Connecting%20to%20Kelvin2/#enabling-multi-factor-authentication","title":"Enabling Multi-Factor Authentication","text":"<p>To enable Multi-Factor Authentication (MFA):</p> <ul> <li>Log into Kelvin2 and type the following command into your terminal to generate a QR Code and key     <pre><code>/opt/flight/bin/flight mfa generate\n</code></pre></li> <li>Using the authenticator application on your mobile device, scan the QR code or enter the key</li> <li>Your authenticator should now be generating one-time passwords to access Kelvin2</li> </ul> <p>Info</p> <p>If you lose access to your authenticator application and can no longer connect to Kelvin2, contact us and an administrator will reset your MFA settings.</p>"},{"location":"Connecting%20to%20Kelvin2/#warnings-when-reconnecting-to-kelvin2","title":"Warnings when reconnecting to Kelvin2","text":"<p>When connecting to Kelvin2 you will be directed to one of four login nodes. Depending on your local settings, you may receive a warning on your terminal when you are directed to a login node that you have not connected to before: <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@       WARNING: POSSIBLE DNS SPOOFING DETECTED!          @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nThe ECDSA host key for kelvin2.qub.ac.uk has changed,\nand the key for the corresponding IP address 143.117.27.19\nis unknown. This could either mean that\nDNS SPOOFING is happening or the IP address for the host\nand its host key have changed at the same time.\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\n</code></pre> To prevent this warning, navigate to the <code>known_hosts</code> file on your local computer (default for linux is <code>~/.ssh/known_hosts</code> and paste the following four lines: <pre><code>kelvin2.qub.ac.uk,143.117.27.19 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBH1T8XlKSmbuHOn0eEIVHfrvzYDBm0G6i2ansLID5XKtedN3OoxU/PqL6glR9pHhN5TinVgOsYYjX+YxlULwoxs=\nkelvin2.qub.ac.uk,143.117.27.20 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBLNqmr6N1XCVdDlkvnI+qxO8QMPsyYPk3zd/CmgKDdgDgdn7rCpJRR3qBuiRjTM0Ok/GWzYk/h8Axaba0CVpv30=\nkelvin2.qub.ac.uk,143.117.27.21 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBAHWMgZWOmETQjmych3RrxMyVcQgtVa1ndkrFbUpiFiP7aiZoVAcacyoGImJWMjKCU+ihkTtREXDz4EDDrMEce4=\nkelvin2.qub.ac.uk,143.117.27.22 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBNQm41eL7A0QoTt9nwMz6gPZxw1L0i379r6f8lNQczoSQuLG9yp1M6ei7S0L6VwquBRkIMdmHzF4HtXmt33wy4k=\n</code></pre></p>"},{"location":"Connecting%20to%20Kelvin2/#connecting-to-kelvin2-using-ssh-clients","title":"Connecting to Kelvin2 using SSH clients","text":"<p>For Windows users, the recommended way to connect to Kelvin-2 is via a SSH client. The two most popular options are MobaXterm and PuTTY.</p>"},{"location":"Connecting%20to%20Kelvin2/#mobaxterm","title":"MobaXterm","text":"<p>MobaXterm is a highly recommended SSH client for Windows. After installing and opening this program, follow these steps to configure the remote session:</p> <ol> <li> <p>On the initial screen, click \"Session\" on the ribbon at the top left of the screen.</p> <p></p> </li> <li> <p>On the \"Sessions\" screen, click on \"SSH\".</p> <p></p> </li> <li> <p>On the \"SSH\" screen, fill the following fields:</p> <ul> <li>Remote host: <code>kelvin2.qub.ac.uk</code> from inside the QUB network, or <code>login.kelvin.alces.network</code> from outside the QUB network.</li> <li>Tick \"Specify username\", and fill the box with your Kelvin-2 username.</li> <li>Port: <code>22</code> from inside the QUB network, or <code>55890</code> from outside the QUB network.</li> </ul> <p></p> </li> <li> <p>Click on \"Advanced SSH settings\" screen and fill the following fields:</p> <ul> <li>Tick \"X11-Forwarding\". This will allow to open graphical applications.</li> <li>For the \"SSH-browser type\", select \"SCP (Enhanced Speed)\"</li> <li>Only if you are connecting from ouside the QUB campus, tick \"Use private key\". Then click on the document icon on the right of the box, and select your private key file <code>my-kelvin-key</code>.</li> <li>Click the \"OK\" box on the bottom of the Window.</li> </ul> <p>The session will be stored for future use, and it will appear on the left part of the initial MobaXTerm window, under the tab \"Sessions\". To connect again, just click on the session.</p> <p></p> </li> </ol> <p>Note</p> <p>If MFA is enabled and \"SSH-browser type\" is set to SFTP you will be receive a verification prompt at every file transfer. For this reason, we recommend setting your \"SSH-browser\" type to SCP (Enhanced Speed)</p>"},{"location":"Connecting%20to%20Kelvin2/#putty","title":"PuTTY","text":"<p>PuTTY can be installed from the Microsoft store, or  downloaded from the web.</p> <p>To use PuTTy as a remote SSH client, the configuration is similar to MobaXterm. To begin, you will need to configure a session:</p> <p>If you are connecting from outside the QUB campus, you must convert the private key <code>my-kelvin-key</code> to the PuTTy Private Key format <code>.ppk</code>. To do so, you need to open the component \"PuTTygen\", and follow the steps</p> <ol> <li> <p>Click on \"Actions - Load an existing private key file - Load\".</p> <p></p> </li> <li> <p>When the browse window opens, change the file format from \"PuTTY Private Key Files (*.ppk)\" to \"All files (*.*)\". Then, browse in your system and select your private-key file <code>my-kelvin-key</code>. You will be asked for the passphrase.</p> </li> </ol> <p>Select \"Save private key\", and save it under the name <code>my-kelvin-key.ppk</code>.</p> <p>Now, you can configure your session in PuTTy, following the steps:</p> <ol> <li> <p>On the initial window, fill the fields:</p> <ul> <li>Host name (or IP address): <code>&lt;username&gt;@kelvin2.qub.ac.uk</code> to connect from inside the QUB campus, or <code>&lt;username&gt;@login.kelvin.alces.network</code> from outside the QUB campus.</li> <li>Port: <code>22</code> to connect from inside the QUB campus, or <code>55890</code> from outside the QUB campus.</li> <li>Connection type: click on \"SSH\".</li> <li>Saved sessions: give a name to your session and click on \"Save\".</li> </ul> <p></p> </li> <li> <p>Now click on the tab \"Connection - SSH - X11\". Tick on \"Enable X11 forwarding\", this will allow to open graphical applications.</p> <p></p> </li> <li> <p>Only if you are connecting from outside the QUB campus, click on the tab \"Connection - SSH - Auth\". At the bottom, go to the box \"Private key file for authentication\", and click on \"Browse\". Select your private-key file <code>my-kelvin-key.ppk</code>.</p> <p></p> </li> <li> <p>Go back to the tab \"Session\" and save the created session, it will appear in the big box. To connect, select your session from the box, and click on \"Open\" at the bottom of the window.</p> </li> </ol>"},{"location":"Getting%20Started/","title":"Getting started","text":"<p>Here we will detail the first steps on accessing our HPC cluster, Kelvin2.</p>"},{"location":"Getting%20Started/#applying-for-an-account","title":"Applying for an account","text":"<p>To gain an account to access Kelvin2, please fill the form found here : </p> <p>https://www.ni-hpc.ac.uk/Access/</p> <p>Please allow 48 hours for your account to be created. Users will be emailed confirmation when their account has been processed. </p>"},{"location":"Getting%20Started/#credentials","title":"Credentials","text":""},{"location":"Getting%20Started/#username","title":"Username","text":"<ul> <li>QUB - Username will be your student/staff number.</li> <li>UU &amp; EPSRC - Username will be the first letter of your first name, followed by your surname in lowercase e.g Joe Bloggs -&gt; jbloggs</li> </ul>"},{"location":"Getting%20Started/#password","title":"Password","text":"<ul> <li>QUB - Password will be that which is associated with QUB AD and QOL.</li> <li>UU &amp; EPSRC - Password will be created when setting up remote access. </li> </ul>"},{"location":"Getting%20Started/#connecting-to-kelvin2","title":"Connecting to Kelvin2","text":"<p>Connection to Kelvin2 is via ssh.  There are 2 ways to connect to Kelvin2: 1. QUB network 2. Remote access</p>"},{"location":"Getting%20Started/#qub-network","title":"QUB Network","text":"<p>If you are within a QUB network you can connect to kelvin2 using your credentials and the following ssh command : </p> <pre><code>ssh &lt;username&gt;@kelvin2.qub.ac.uk\n</code></pre>"},{"location":"Getting%20Started/#remote-access","title":"Remote Access","text":"<p>Kelvin2 can be accessed from outside the QUB network by using SSH keys.   - Kelvin server name: login.kelvin.alces.network     - Port: 55890</p> <p>Remote access from a Linux computer <ul> <li>Open a terminal and create a public/private key pair on the remote machine:<pre><code>ssh-keygen -t rsa \u2013f ~/.ssh/my-kelvin-key\n</code></pre> </li> </ul> <p>All users MUST set a password for the passphrase when prompted.</p> <ul> <li>Copy the public key generated to Kelvin2</li> </ul> <p>The easiest way to do this is to place the  contents of the public key file into the authorized users file in your Kelvin2 home directory.</p> <pre><code>cat ~/.ssh/my-kelvin-key.pub\n</code></pre> <p>Login to Kelvin2 and paste the above output into the end of the file:</p> <pre><code> ~/.ssh/authorized_keys\n</code></pre> <p>Note: if you are away from the QUB campus and hence do not have access to Kelvin2,  send the public key via a method found  here,  whereupon an administrator will add it for you.</p> <ul> <li>Login using SSH<pre><code>ssh \u2013p 55890 \u2013i ~/.ssh/my-kelvin-key &lt;username&gt;@login.kelvin.alces.network\n</code></pre> </li> </ul> <p>Remote access from a Windows computer <p>Two popular terminal emulators are putty and MobaXTerm. If using putty there is a tool called puttygen which will generate public/private keys.</p> <ul> <li>Open a terminal and create a public/private key pair on the remote machine:<pre><code>ssh-keygen\n</code></pre> </li> </ul> <p>Call your key <code>my-kelvin-key</code>.  All users MUST set a password for the passphrase when prompted.</p> <ul> <li>Copy the public key to Kelvin2.</li> </ul> <p>The easiest way to do this is to place the contents of the public key file into the authorized users file in your Kelvin2 home directory.</p> <pre><code>type my-kelvin-key.pub\n</code></pre> <p>Login to Kelvin2 and paste the above output into the end of the file:</p> <pre><code> ~/.ssh/authorized_keys\n</code></pre> <p>Note: if you are away from the QUB campus and hence do not have access to Kelvin2,  send the public key via a method found  here,  whereupon an administrator will add it for you.</p> <ul> <li>Login using SSH<pre><code>ssh \u2013p 55890 \u2013i ~/.ssh/my-kelvin-key &lt;username&gt;@login.kelvin.alces.network\n</code></pre> </li> </ul>"},{"location":"Getting%20Started/#ssh-clients","title":"SSH clients","text":"<p>For Windows users, the recommended way to connect to Kelvin-2 is via a ssh client. The two most popular options are MobaXTerm and PuTTy.</p>"},{"location":"Getting%20Started/#mobaxterm","title":"MobaXTerm","text":"<p>MobaXTerm is the recommended ssh client. It can be downloaded from the web  https://mobaxterm.mobatek.net</p> <p>To configure the remote session, firstly download and install MobaXTerm. Then, open it and follow the steps:</p> <ol> <li> <p>On the initial screen, click on \"Sessions\".</p> <p> </p> </li> <li> <p>On the \"Sessions\" screen, click on \"SSH\".</p> <p></p> </li> <li> <p>On the \"SSH\" screen, fill the following fields:</p> <ul> <li>Remote host: <code>kelvin2.qub.ac.uk</code> from inside the QUB campus, or <code>login.kelvin.alces.network</code> from outside the QUB campus.</li> <li>Tick \"Specify username\", and fill the box with your Kelvin-2 username.</li> <li>Port: <code>22</code> from inside the QUB campus, or <code>55890</code> from outside the QUB campus.</li> </ul> <p>Click on \"Advanced SSH Settings\"</p> <p></p> </li> <li> <p>On the \"Advanced SSH Settings\" screen, fill the following fields:</p> <ul> <li>Tick \"X11-Forwarding\". This will allow to open graphical applications.</li> <li>Only if you are connecting from ouside the QUB campus, tick \"Use private key\". Then click on the \"paper sheet\" icon on the right of the box, and select your private key file <code>my-kelvin-key</code>.</li> <li>Click the \"OK\" box on the bottom of the Window.</li> </ul> <p>The session will be stored for future use, and it will appear on the left part of the initial MobaXTerm window, under the tab \"Sessions\". To connect again, just click on the session.</p> <p></p> </li> </ol>"},{"location":"Getting%20Started/#putty","title":"PuTTy","text":"<p>PuTTy can be installed from the Microsoft store, or downloaded from the web : https://www.putty.org</p> <p>To use PuTTy as a remote ssh client, the configuration is similar to MobaXTerm. To begin, you will need to configure a session:</p> <p>If you are connecting from outside the QUB campus, you must convert the private key <code>my-kelvin-key</code> to the PuTTy Private Key format <code>.ppk</code>. To do so, you need to open the component \"PuTTygen\", and follow the steps</p> <ol> <li> <p>Click on \"Actions - Load an existing private key file - Load\".</p> <p></p> </li> <li> <p>When the browse window opens, change the file format from \"PuTTy Private Key Files (*.ppk)\" to \"All files (*.*)\". Then, browse in your system and select your private-key file <code>my-kelvin-key</code>. You will be asked for the passphrase. Select \"Save private key\", and save it under the name <code>my-kelvin-key.ppk</code>.</p> </li> </ol> <p>Now, you can configure your session in PuTTy, following the steps:</p> <ol> <li> <p>On the initial window, fill the fields:</p> <ul> <li>Host name (or IP address): <code>&lt;username&gt;@kelvin2.qub.ac.uk</code> to connect from inside the QUB campus, or <code>&lt;username&gt;@login.kelvin.alces.network</code> from outside the QUB campus.</li> <li>Port: <code>22</code> to connect from inside the QUB campus, or <code>55890</code> from outside the QUB campus.</li> <li>Connection type: click on \"SSH\".</li> <li>Saved sessions: give a name to your session and click on \"Save\".</li> </ul> <p></p> </li> <li> <p>Now click on the tab \"Connection - SSH - X11\". Tick on \"Enable X11 forwarding\", this will allow to open graphical applications.</p> <p></p> </li> <li> <p>Only if you are connecting from outside the QUB campus, click on the tab \"Connection - SSH - Auth\". At the bottom, go to the box \"Private key file for authentication\", and click on \"Browse\". Select your private-key file <code>my-kelvin-key.ppk</code>.</p> <p></p> </li> <li> <p>Go back to the tab \"Session\" and save the created session, it will appear in the big box. To connect, select your session from the box, and click on \"Open\" at the bottom of the window.</p> </li> </ol>"},{"location":"Getting%20Started/#copying-data-to-kelvin2","title":"Copying data to Kelvin2","text":"<p>To copy files across to kelvin use pscp or scp. Give the full path to the directory/file to be copied and connect using your credentials, like below:</p> <p>Windows OS with putty installed</p> <pre><code>pscp \u2013p 55890 \"C:\\Users\\MyPC\\Documents\\test.txt\" -i my_kelvin_key &lt;username&gt;@login.kelvin.alces.network\n</code></pre> <p>Linux OS:</p> <p>Attention that the <code>-P</code> flag to specify the ssh port must be in capital</p> <pre><code>scp \u2013P 55890 -i ~/.ssh/my_kelvin_key ~/test.txt &lt;username&gt;@login.kelvin.alces.network:~\n</code></pre>"},{"location":"Getting%20Started/#windows-scp-clients","title":"Windows scp clients","text":"<p>In Windows machines, MobaXTerm and WinSCP can be used to transfer files from and to Kelvin-2 in a graphical easy way. Already saved sessions of MobaXTerm or PuTTy can be used.</p> <ul> <li> <p>MobaXTerm:   https://mobaxterm.mobatek.net</p> </li> <li> <p>WinSCP:  https://winscp.net/eng/index.php</p> </li> </ul>"},{"location":"Job%20Submission/","title":"Job Submission","text":""},{"location":"Job%20Submission/#ni-hpc-system-diagram","title":"NI-HPC System Diagram","text":""},{"location":"Job%20Submission/#job-handling","title":"Job Handling","text":""},{"location":"Job%20Submission/#slurm","title":"Slurm","text":"<p>Jobs on the cluster are under the control of a scheduling system, Slurm.</p> <p>Jobs are scheduled according to the currently available resources and the resources that are requested. Information on how to request resources for your job are outlined here.</p> <p>Jobs are not necessarily run in the order in which they are submitted. Jobs needing a large number of cores, memory or walltime will have to queue until the requested resources become available in. The system will run smaller jobs, that can fit in available gaps, until all of the resources that have been requested for the larger job become available. Always run jobs with the specific number of resources needed."},{"location":"Job%20Submission/#submitting-a-job","title":"Submitting a Job","text":"<p>There are 2 classes of jobs that can be ran on Kelvin2.</p> <ul> <li>Non-interactive - <code>sbatch</code></li> <li>Interactive - <code>srun</code></li> </ul>"},{"location":"Job%20Submission/#sbatch","title":"sbatch","text":"<p>Jobs are submitted via a job-script To learn more about writing a jobscript see here.</p> <p>Once you have created your jobscript you then submit it using the <code>sbatch</code> command and its name :</p> <pre><code>sbatch my_jobscript.sh\n</code></pre> <p>Once your job is submitted you will recieve a unique <code>JOBID</code>.</p>"},{"location":"Job%20Submission/#srun","title":"srun","text":"<p>srun is an interactive job - allows users to run interactive applications directly on a compute node.</p> <p>To start an interactive job :</p> <pre><code>srun --pty /bin/bash\n</code></pre> <ul> <li>Users should specify resources required to run.</li> <li>Input data is the shell session or application started.</li> <li>Output data is shown on screen or can be specified to write elsewhere. </li> </ul>"},{"location":"Job%20Submission/#queue-status","title":"Queue status","text":"<p>Once you have submitted your jobs using <code>sbatch</code> or <code>srun</code>, you can then view your queue status to see how your jobs are doing along with further information:</p> <pre><code>squeue -u &lt;username&gt;\n</code></pre> <p>Example <code>squeue</code> output :</p> <pre><code>JOBID PARTITION NAME       USER  ST TIME NODES NODELIST(REASON)\n 11    all       mpiJob    user1 PD 0:00 2     node101, node102\n 2     all       serialJob user1 R  0:02 1     node101\n</code></pre>"},{"location":"Job%20Submission/#job-states","title":"Job states","text":"<p>Once you have executed the <code>squeue</code> command take note of the current state (ST) :</p> <ul> <li>Running jobs (R) - your job is currently running in the compute.</li> <li>Queuing jobs (PD) - your job is waiting for resources to become available to run</li> <li>Failed jobs (F)- your job submission has failed and should be delted from the queue.</li> </ul>"},{"location":"Job%20Submission/#deleting-jobs","title":"Deleting jobs","text":"<p>If you need to delete a job from the current queue you can use the <code>scancel</code> command with the unique <code>JOBID</code> of the job.</p> <pre><code>scancel 8\n</code></pre> <p>Users can delete their own jobs only.</p>"},{"location":"Job%20Submission/#slurm-cheat-sheet","title":"Slurm cheat sheet","text":"<p>Common job commands </p> SGE Slurm Submit a job qsub  sbatch  Delete a job qdel  scancel  Job status (all) qstat showq squeue Job status (by job) qstat -j  squeue -j  Job status (detailed) qstat -j  scontrol show job  Show expected start time qstat -j  squeue -j  --start Start an interactive job qrsh srun --pty  /bin/bash Monitor jobs resource usage qacct -j  sacct -j  --format=\"JobID,jobname,NTasks,nodelist,CPUTime,ReqMem,MaxVMSize,Elapsed\" <p>Slurm Environmental variables</p> Variable Function SLURM_ARRAY_JOB_ID Job array's master job ID number. SLURM_ARRAY_TASK_ID Job array ID (index) number. SLURM_CLUSTER_NAME Name of the cluster on which the job is executing. SLURM_CPUS_PER_TASK Number of cpus requested per task. Only set if the --cpus-per-task option is specified. SLURM_JOB_ACCOUNT Account name associated of the job allocation. SLURM_JOB_ID The ID of the job allocation. SLURM_JOB_NAME Name of the job. SLURM_JOB_NODELIST List of nodes allocated to the job. SLURM_JOB_NUM_NODES Total number of nodes in the job's resource allocation. SLURM_JOB_PARTITION Name of the partition in which the job is running. SLURM_JOB_UID The ID of the job allocation. See SLURM_JOB_ID. Included for backwards compatibility. SLURM_JOB_USER User name of the job owner SLURM_MEM_PER_CPU Same as --mem-per-cpu SLURM_MEM_PER_NODE Same as --mem SLURM_NTASKS Same as -n, --ntasks SLURM_NTASKS_PER_NODE Number of tasks requested per node. Only set if the --ntasks-per-node option is specified. SLURM_PROCID The MPI rank (or relative process ID) of the current process. <p>More information about  Slurm commands, flags and environment variables, can be found in the  Slurm web page.</p>"},{"location":"Job%20Submission/#optimization","title":"Optimization","text":"<ul> <li>Never run jobs on the login nodes. That will seriously disturb other users who are logged in. Login nodes are never to run jobs.</li> <li>Allocate interactive sessions with <code>srun</code> if you need to run a job interactively, never use the login nodes.</li> <li>Request the necessary resources for your job. Use the job analysis tool <code>sacct</code> to check if you allocated more memory than the necessary for futures jobs with the same application.</li> <li>Don't allocate more resources than necessary, that will increase the queue time, and will waste resources of the machine that could be used by other users.</li> <li>Try to spread the allocation among several nodes, 20 CPUs and 100 Gb of memory is a reasonable amount to be allocated in a single node.</li> <li>Don't allocate a big number of CPUs or a large amount of memory in a single node.</li> <li>Give freedom to Slurm to distribute the resources among the nodes, use the flags <code>--ntasks</code> and <code>--mem-per-cpu</code> to allocate resources per CPU preferably than per node.</li> <li>Don't restrict the resources per node if possible, do not use the flags <code>--ntasks-per-node</code> nor <code>--mem</code>.</li> <li>Never allocate more resources than available in the nodes. Review the training material for information about the resources per node in each partition.</li> <li>Allocate always the correct partition, double check the resources you require, in particular the wall time, and fit it in the particular partition. Check the partition table in the training material.</li> <li>Specify an error output different to the standard output. In case the job crashes, this output has useful information about why the job failed, so how it can be fix.</li> <li>Activate email notifications.</li> </ul>"},{"location":"Kelvin2%20Hardware/","title":"Kelvin2 Hardware","text":""},{"location":"Kelvin2%20Hardware/#cpu-nodes","title":"CPU Nodes","text":"<p>AMD Nodes</p> <ul> <li>Nodes: node[101-209]</li> <li>Server: Dell PowerEdge R6525</li> <li>CPU: 2x AMD EPYC 7702 64-Core Processor</li> <li>Sockets:Cores:Threads: 2:64:1</li> <li>Memory:<ul> <li>node[101-160]: 768GiB</li> <li>node[161-182], node[186-209]: 1TiB</li> <li>node[183-185]: 960GiB</li> </ul> </li> <li>NUMA domains:<ul> <li>node[101-133,135-160]: 8</li> <li>node[134,161-209]: 2</li> </ul> </li> <li>SLURM partitions:<ul> <li>node[101-159], node[161-165], node[167-182], node[196-209]: k2-hipri, k2-medpri, k2-lowpri, k2-epsrc</li> <li>node160: k2-sandbox</li> <li>node166, node[183-195]: (reserved for specific research groups)</li> </ul> </li> </ul> <p>AMD X Series Nodes</p> <ul> <li>Nodes: node[301-302]</li> <li>Server: Dell PowerEdge R6525</li> <li>CPU: 2x AMD EPYC 7773X 64-Core Processor</li> <li>Sockets:Cores:Threads: 2:64:1</li> <li>Memory: 1TiB</li> <li>NUMA domains: 8</li> <li>SLURM partitions: k2-amd-xseries</li> </ul>"},{"location":"Kelvin2%20Hardware/#gpu-nodes","title":"GPU Nodes","text":"<p>V100 nodes</p> <ul> <li>Nodes: gpu[103-110]</li> <li>Server: Dell EMC DSS 8440</li> <li>CPU: 2x Intel Xeon Platinum 8168 CPU @ 2.70GHz</li> <li>Sockets:Cores:Threads: 2:24:1</li> <li>Memory: 512GiB</li> <li>NUMA domains: 2</li> <li>GPU: 4x Tesla V100 PCIe 32GB</li> <li>SLURM partitions: k2-gpu, k2-gpu-interactive, k2-epsrc</li> </ul> <p>A100 nodes</p> <ul> <li>Nodes: gpu[111-113]</li> <li>Server: Dell PowerEdge XE8545</li> <li>CPU: 2x AMD EPYC 7763 64-Core Processor</li> <li>Sockets:Cores:Threads: 2:64:1</li> <li>Memory: 1TiB</li> <li>NUMA domains: 8</li> <li>GPU: 4x Tesla A100 SXM4 80GB</li> <li>SLURM partitions: k2-gpu, k2-gpu-interactive, k2-epsrc</li> </ul> <p>A100 slice node</p> <ul> <li>Nodes: gpu[114]</li> <li>Server: Dell PowerEdge XE8545</li> <li>CPU: 2x AMD EPYC 7763 64-Core Processor</li> <li>Sockets:Cores:Threads: 2:64:1</li> <li>Memory: 1TiB</li> <li>NUMA domains: 8</li> <li>GPU: 4x Tesla A100 SXM4 80GB, each partitioned into 7 instances</li> <li>SLURM partitions: k2-gpu, k2-gpu-interactive, k2-epsrc</li> </ul>"},{"location":"Kelvin2%20Hardware/#high-memory-nodes","title":"High Memory Nodes","text":"<ul> <li>Nodes: smp[106-113]</li> <li>Server: Dell PowerEdge R6525</li> <li>CPU: 2x AMD EPYC 7702 64-Core Processor</li> <li>Sockets:Cores:Threads:<ul> <li>smp[106-107]: 8:16:2</li> <li>smp[108-113]: 2:64:1</li> </ul> </li> <li>Memory: 2TiB</li> <li>NUMA domains: <ul> <li>smp[106-109]: 8</li> <li>smp[110-113]: 2</li> </ul> </li> <li>SLURM partitions: k2-himem, k2-epsrc-himem</li> </ul>"},{"location":"Kelvin2%20Hardware/#legacy-cpu-nodes","title":"Legacy CPU Nodes","text":"<p>Type 1</p> <ul> <li>Nodes: node[001-024], node[026-034], node[036-039], node[041], node[045-051], node[053-061]</li> <li>Server: ProLiant XL170r Gen9</li> <li>CPU: Intel Xeon CPU E5-2660 v3 @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:10:1</li> <li>Memory:<ul> <li>node[001-014]: 256GiB</li> <li>All except node[001-014]: 128GiB</li> </ul> </li> <li>NUMA domains: 2</li> <li>SLURM partitions: <ul> <li>all except node[061]: hipri, medpri, lowpri</li> <li>node[061]: sandbox</li> </ul> </li> </ul> <p>Type 2</p> <ul> <li>Nodes: node[071-079]</li> <li>Server: ProLiant XL170r Gen10</li> <li>CPU: Intel Xeon Gold 6126 CPU @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:12:1</li> <li>Memory:<ul> <li>node[071-073]: 384GiB</li> <li>node[074-079]: 192GiB</li> </ul> </li> <li>NUMA domains: 2</li> <li>SLURM partitions: hipri, medpri, lowpri, avx512</li> </ul>"},{"location":"Kelvin2%20Hardware/#legacy-gpu-nodes","title":"Legacy GPU Nodes","text":"<p>K4200 node</p> <ul> <li>Nodes: gpu01</li> <li>Server: ProLiant DL380 Gen9</li> <li>CPU: 2x Intel Xeon CPU E5-2640 v3 @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:8:1</li> <li>Memory: 32GiB</li> <li>NUMA domains: 2</li> <li>GPU: Quadro K4200</li> <li>SLURM partitions: gpu</li> </ul> <p>V100 node</p> <ul> <li>Nodes: gpu02</li> <li>Server: ProLiant XL270d Gen10</li> <li>CPU: 2x Intel Xeon Gold 6126 CPU @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:12:1</li> <li>Memory: 768GiB</li> <li>NUMA domains: 2</li> <li>GPU: 4x Tesla V100 SXM2 32GB</li> <li>SLURM partitions: gpu</li> </ul>"},{"location":"Kelvin2%20Hardware/#legacy-high-memory-nodes","title":"Legacy High Memory Nodes","text":"<p>Type 1</p> <ul> <li>Nodes: smp01</li> <li>Server: ProLiant DL560 Gen9</li> <li>CPU: 4x Intel Xeon CPU E5-4627 v3 @ 2.60GHz</li> <li>Sockets:Cores:Threads: 4:10:1</li> <li>Memory: 1TiB</li> <li>NUMA domains: 4</li> <li>SLURM partitions: himem</li> </ul> <p>Type 2</p> <ul> <li>Nodes: smp[03-04]</li> <li>Server: ProLiant DL360 Gen10</li> <li>CPU: 2x Intel Xeon Gold 6126 CPU @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:12:2</li> <li>Memory: 1TiB</li> <li>NUMA domains: 2</li> <li>SLURM partitions: himem</li> </ul> <p>Type 3</p> <ul> <li>Nodes: smp05</li> <li>Server: ProLiant DL360 Gen10</li> <li>CPU: 2x Intel Xeon Gold 6130 CPU @ 2.10GHz</li> <li>Sockets:Cores:Threads: 2:16:1</li> <li>Memory: 768TiB</li> <li>NUMA domains: 2</li> <li>SLURM partitions: himem</li> </ul>"},{"location":"Kelvin2%20Overview/","title":"Kelvin2 Overview","text":"<p>The NI-HPC centre hosts the Kelvin-2 research cluster. Kelvin2 runs on a Linux (Centos 7) Operating system. Below we will list the resources available.</p>"},{"location":"Kelvin2%20Overview/#compute","title":"Compute","text":"<ul> <li>96 x 128 core Dell PowerEdge R6525 compute nodes with AMD EPYC 7702 dual 64-Core Processors (786GB RAM).</li> <li>8 High memory nodes (2TB RAM).</li> <li>32 x NVIDIA Tesla v100 GPUs in 8 nodes.</li> <li>16 x NVIDIA Tesla A100 GPUs in 4 nodes.</li> <li>2PB of lustre parallel file system for scratch storage.</li> </ul>"},{"location":"Kelvin2%20Overview/#storage","title":"Storage","text":"<p>There are 4 pools of storage on Kelvin2 :</p>"},{"location":"Kelvin2%20Overview/#home","title":"Home","text":"<pre><code>/users/\"username\"\n</code></pre> <ul> <li>Default place that users login to.</li> <li>50GB/100K file quota.</li> <li>No automated file deletion - therefore good to store smaller/compressed longer term data.</li> </ul>"},{"location":"Kelvin2%20Overview/#scratch","title":"Scratch","text":"<pre><code>/mnt/scratch2/users/\"username\"\n</code></pre> <ul> <li>Large, shared storage area</li> <li>No Quota</li> <li>Temporary data solution - once per month a purge will delete any data that hasnt been accessed in the previous 3 months.</li> <li>Mainly used for storing data that is neccassary to running jobs and storing their output.</li> <li>Once results are output, permanent data should be moved to longer term storage ( Home/McClayRDS ).</li> </ul>"},{"location":"Kelvin2%20Overview/#temp","title":"Temp","text":"<pre><code>/tmp\n</code></pre> <ul> <li>Local scratch disk on nodes.</li> <li>Data will be automatically deleted when session ends.</li> </ul>"},{"location":"Kelvin2%20Overview/#mcclayrds","title":"McClayRDS","text":"<pre><code>/mnt/autofs/mcclayrds-projects\n</code></pre> <ul> <li>QUB users only</li> <li>Available only from  login and data movement nodes, not compute nodes.</li> <li>Group quotas enabled.</li> <li>Replicated to secondary site.</li> <li>To apply for storage on McClayRDS, contact the Research Data Management team here.</li> </ul>"},{"location":"Modules%20%26%20Jobscripts/","title":"Modules & Jobscripts","text":""},{"location":"Modules%20%26%20Jobscripts/#modules-enivornment","title":"Modules Enivornment","text":"<p>Kelvin2 has a large repository of Software installed directly on the system. There are multiple versions of different software for users to load into their environment and make use of when running their jobs.</p>"},{"location":"Modules%20%26%20Jobscripts/#module-avail","title":"Module avail","text":"<p>The <code>module avail</code> command will show the list of applications that are currently installed on the central repository.</p> <p></p>"},{"location":"Modules%20%26%20Jobscripts/#module-load","title":"Module load","text":"<p>The <code>module load</code> command will load the selected module, including its binaries/libraries into the users current environment.</p> <p></p>"},{"location":"Modules%20%26%20Jobscripts/#module-list","title":"Module list","text":"<p>The <code>module list</code> command will show the currently loaded modules in the users environment.</p> <p></p>"},{"location":"Modules%20%26%20Jobscripts/#other-module-commands","title":"Other module commands","text":"<ul> <li><code>module unload &lt;module&gt;</code> - Removes the module from the current environment</li> <li><code>module display &lt;module&gt;</code> -  Shows information about the software</li> </ul>"},{"location":"Modules%20%26%20Jobscripts/#jobscripts","title":"Jobscripts","text":"<p>The most popular way of running jobs on a HPC cluster is via batch (submitting a jobscript).</p> <ul> <li>Contains the commands to run your job</li> <li>Can contain instructions for the job-scheduler</li> </ul> <p>Below we will outline how to create a jobscript.</p>"},{"location":"Modules%20%26%20Jobscripts/#first-steps","title":"First steps","text":"<p>To create a jobscript we first must open a text editor, in this example we will us VI. (https://www.tutorialspoint.com/unix/unix-vi-editor.htm) </p> <p>To begin, give your jobscript a name:</p> <pre><code>vi my_jobscript.sh\n</code></pre> <p>At the start of each jobscript you must include the \"shebang\":</p> <pre><code>#!/bin/bash\n</code></pre>"},{"location":"Modules%20%26%20Jobscripts/#job-scheduler-instructions","title":"Job scheduler instructions","text":"<p>Once you have given your jobscript a name and added the shebang you can then start to fill it with multiple instructions to pass to the Job Scheduler.</p> <p>The most popular of these instructions are :</p> <ul> <li>Job name</li> <li>Output name/location</li> <li>Email Notifications</li> <li>Resource Request - CPU, Memory, Runtime</li> </ul> <p>Each instruction must come after the identifier <code>#SBATCH</code>.</p>"},{"location":"Modules%20%26%20Jobscripts/#email-output-jobname","title":"Email, Output, Jobname","text":"<pre><code>    #!/bin/bash\n\n    #SBATCH --job-name=new_job    ## this will give your jobscript a name\n\n    #SBATCH --output=/users/&lt;username&gt;/test.output   ##This will be the location and name of your output\n\n    #SBATCH --mail-user=&lt;email@work.com&gt;\n\n    #SBATCH --mail_type=ALL  ##This will set up automatic emails if/when your job begins, ends or fails.\n</code></pre>"},{"location":"Modules%20%26%20Jobscripts/#resource-request","title":"Resource request","text":"<p>Default resources when jobs resources are not requested are as follows :</p> <ul> <li>1 CPU Core</li> <li>1GB of memory per core</li> <li>Max runtime of 3hrs</li> </ul> <p>If you need more than the default they can request using the instructions below :</p> <pre><code>    #SBATCH --ntasks=&lt;cores&gt;   ## this requests the number of cpu cores needed for the job\n\n    #SBATCH --nodes=&lt;nodes&gt;  ## this requests the maximum number of nodes for cores to be split over\n\n    #SBATCH --mem-per-cpu=&lt;memory&gt;   ## this requests the amount of memory per core needed for the job - can be specified in K (kilobyte), M (Megabyte), G (gigabyte)\n\n    #SBATCH --time=&lt;time&gt;   ## this requests the maximum runtime for a job. (hh:mm:ss)\n\n    #SBATCH --partition=&lt;partitions&gt;   ## This requests the correct partition needed for a job e.g within the correct timeframe or a special purpose node ( GPU or hi-memory)\n</code></pre> <p>Paritions</p> <ul> <li>k2-sandbox \u2013 run a quick job to test if script will run (Max runtime - 30 mins).</li> <li>k2-hipri - run a job under 3 hours (default)</li> <li>k2-medpri - run a job that will run longer than 3 hours but  less than 24 hours.</li> <li>k2-lowpri - run a job that will run longer than 24 hours but  less than 10 days.</li> <li>k2-gpu \u2013 run a job on a GPU node (Max runtime - 3 days).</li> <li>k2-himem \u2013 run a job on a high memory node (Max runtime - 3 days).</li> <li>avx512 \u2013 run a job on an avx512 enabled node.</li> <li>k2-esprc \u2013 run a job on esprc nodes (only esprc users).</li> </ul> <p>#### Full example jobscript</p> <p>Once you have completed giving your jobscript the correct scheduler instructions you can then move onto loading the correct applications to your environment &amp; running any commands.</p> <p>Below is a full example jobscript:</p> <pre><code>    #!/bin/bash\n\n    #SBATCH --job-name=new_job\n\n    #SBATCH --output=/users/3052732/test.output\n\n    #SBATCH --mail-user=email@work.com\n\n    #SBATCH --mail-type=ALL\n\n    #SBATCH --ntasks=1\n\n    #SBATCH --mem-per-cpu=1G\n\n    #SBATCH --time=02:45:00\n\n    #SBATCH --partition=k2-hipri\n\n    module load apps/python3/3.10.5/gcc-9.3.0\n\n    python3 test_job.py\n</code></pre>"},{"location":"Modules%20%26%20Jobscripts/#gpus","title":"GPU's","text":"<p>Available GPU hardware :</p> Nodes GPU type GRES type Total Partition gpu01 1 x Quadro K4200 k4200 1 gpu bio-gpu gpu02 4 x Tesla V100 SXM2 32GB v100 4 bio-gpu gpu103-110 4 x Tesla V100 PCIe 32GB v100 32 k2-gpu k2-epsrc-gpu gpu111-113 4 x Tesla A100 SXM4 80GB a100 12 k2-gpu k2-epsrc-gpu gpu114 28 x CI slices of A100 1g.10gb 28 k2-gpu k2-epsrc-gpu"},{"location":"Modules%20%26%20Jobscripts/#gpu-resource-allocation","title":"GPU resource allocation","text":"<p>To allocate GPU resources, you need to specify it with flag :</p> <pre><code>#SBATCH --gres\n</code></pre> <p>and the specific resource, for example:</p> <ul> <li>Tesla V100 PCIe 32 Gb, full GPU only: <code>--gres gpu:v100:1</code></li> <li>Tesla A100 SXM4 80GB, full GPU: <code>--gres gpu:a100:1</code></li> <li>CI slices of the A100 in gpu114: <code>--gres gpu:1g.10gb:1</code></li> </ul> <p>For further job scheduler instructions please see the official slurm documentation.</p>"},{"location":"Quickstart%20Guide/","title":"Quick-Start Guide","text":""},{"location":"Quickstart%20Guide/#login-nodes","title":"Login nodes","text":"<p>After connecting to Kelvin2 you will start your session in one of four login nodes as indicated in the terminal window.</p> <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$\n</code></pre> <p>While using a login node, you are only permitted to do small tasks such as</p> <ul> <li>managing files</li> <li>small data transfers</li> <li>creating, editing and submitting jobscripts</li> <li>checking and managing jobs</li> </ul> <p>Warning</p> <p>Please do not run computationally expensive jobs on a login node. There are only 4 login nodes split between all Kelvin2 users and running high workloads here will seriously impact the other users.</p>"},{"location":"Quickstart%20Guide/#running-your-jobs-on-compute-nodes","title":"Running your jobs on compute nodes","text":"<p>A user wishing to run a job on a compute node must first write a batch script. This batch script contains specifications for the required computational resources, instructions for loading program dependencies, and commands to execute the code. Once the batch script is complete, this is submitted to a job scheduler called SLURM and the job will subsequently be queued and run on a compute node when resources become available. The scheduler ensures that all users get their fair share of system resources. </p> <p>This process is quite different to how you normally run programs on your own personal computer. As such, the following step-by-step guide explains how to run a simple Python 'Hello World' job on a compute node using a batch script submitted to the job scheduler. This walkthrough is designed to be very minimal to provide a first-time user with an overview of the process. More information is contained in the Running Jobs documentation page.</p>"},{"location":"Quickstart%20Guide/#step-by-step-guide-to-submitting-your-first-job-on-kelvin2","title":"Step-by-step guide to submitting your first job on Kelvin2","text":"<p>Click the following steps to expand:</p> 1. Create Python executable file <ul> <li>Enter the following command into your Kelvin2 terminal to create the file for our 'Hello World' program     <pre><code>vi hello-world.py\n</code></pre></li> <li>Copy the following lines of Python code into the file, then save and exit vim.      <pre><code>import os\n\nprint(\"hello world! I am running on\", os.environ['HOSTNAME'])\n</code></pre>     This code will print a hello world message and the name of the compute node the code is running on to the standard output stream. If you were running this program directly, this would be printed to your terminal.</li> </ul> 2. Create SLURM batch script <ul> <li>Enter the following command into your Kelvin2 terminal to create the batch script file that we will use to submit <code>hello-world.py</code> to the job scheduler.     <pre><code>vi hello-world-jobscript.sh\n</code></pre></li> <li> <p>Copy the following lines of code into the file, then save and exit vim.      <pre><code>#!/bin/bash\n\n#SBATCH --output=hello-world-job.output\n#SBATCH --time=00:00:10\n\nmodule load apps/python3/3.10.5/gcc-9.3.0\n\npython3 hello-world.py\n</code></pre>   In this example, the batch script simply specifies:</p> <ul> <li>the file in which the program output will be written</li> <li>a time limit after which the job will be automatically closed if it has not yet completed</li> <li>which Python module should be loaded to run the code</li> <li>a line which uses a bash command to execute our Python program</li> </ul> </li> </ul> 3. Submit the batch script <ul> <li>Enter the following command into your Kelvin2 terminal to submit the batch script to the scheduler. The job will then be queued and ran on a compute node whenever resources become available.   <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$sbatch hello-world-jobscript.sh\nSubmitted batch job &lt;jobid&gt;\n</code></pre></li> </ul> 4. Check Status of Job <ul> <li>Enter the following command into your Kelvin2 terminal to check the status of your submitted jobs.   <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$squeue -u &lt;username&gt;\n</code></pre></li> <li>If you were quick typing the above command, your may see your job status as Running (R) or Pending (PD). If you do not see your job in your queue it means your job is already complete. </li> </ul> 5. View output of job <ul> <li>After the job is complete, enter the following command into your Kelvin2 terminal to view the output of your program.   <pre><code>  [&lt;username&gt;@login1 [kelvin2] ~]$ cat hello-world-job.output\n  hello world! I am running on node167\n</code></pre></li> <li>You have now successfully ran a job on a compute node.</li> </ul>"},{"location":"Quickstart%20Guide/#storing-your-files-on-kelvin2","title":"Storing your files on Kelvin2","text":""},{"location":"Quickstart%20Guide/#home-directory","title":"Home Directory","text":"<p>Your home directory is found at <code>/users/$USER</code>. If you followed the step-by-step guide above, then this is where you have stored <code>hello-world.py</code>, <code>hello-world-jobscript.sh</code> and <code>hello-world-job.output</code>.</p> <p>By default, the contents of your Home directory are private. You can check permissions using the <code>ls</code> command.</p> <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ ls -ald $HOME\ndrwx------ 17 &lt;username&gt; clusterusers 4096 Oct 31 10:12 /users/&lt;username&gt;\n</code></pre> <p>Each user has a quota for how much data (50GB) and how many files (100k) they can store in their home directory. The limit on the number of files appears very large, however some user installations can generate lots of small files (e.g. Anaconda) which can breach this limit.  The quota can be checked using the <code>quota -s</code> command as follows: <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ quota -s\nDisk quotas for user &lt;username&gt; (uid &lt;userid&gt;):\n     Filesystem   space   quota   limit   grace   files   quota   limit   grace\nstorage1:/export/users\n                 36512K  51200M  76800M             292    100k    150k\n</code></pre></p> <p>In this case, the user is using 36512KB out of their 50GB disk space quota and has 292 files out of their 100k file limit quota. </p>"},{"location":"Quickstart%20Guide/#shared-scratch-directory","title":"Shared Scratch Directory","text":"<p>Users also have access to a Shared Scratch directory <code>/mnt/scratch2/users/$USER</code>.</p> <p>The Shared Scratch directory does not have a defined quota for each user and is therefore more suitable for storing larger amounts of data. However, the Shared Scratch directory has a total capacity of 2PB and therefore any data that has not been accessed in the previous 90 days is subject to deletion. </p> <p>By default, the Shared Scratch directory is more open than your Home directory. You can check the permissions using the <code>ls</code> command as follows:</p> <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ ls -ald /mnt/scratch2/users/$USER\ndrwxr-x--- 17 &lt;username&gt; clusterusers 4096 Sep 21 13:52 /mnt/scratch2/users/&lt;username&gt;\n</code></pre> <p>This Shared Scratch directory is typically used for</p> <ul> <li>sharing data between cluster users</li> <li>temporary storage of large amounts of data (either as input to or output from jobs)</li> <li>user installations that generate a large number of small files (e.g. using Anaconda)</li> </ul>"},{"location":"Quickstart%20Guide/#temporary-directory","title":"Temporary Directory","text":"<p>Each compute node has a local scratch directory <code>/tmp</code> for storing data during a job. Although using this storage may result in faster read/write times during a job, this data will automatically be deleted when the job completes. </p> <p>The temporary directory is typically used for </p> <ul> <li>temporary files produced during a job that are not required after job completion</li> <li>files that need to be read/written to multiple times within a job</li> </ul>"},{"location":"Quickstart%20Guide/#mcclay-research-data-storage","title":"McClay Research Data Storage","text":"<p>QUB PIs may also request access to the McClay Research Data Storage (McClayRDS) for the duration of their research project. The storage space will be assigned to individual research projects and access is shared amongst members of that research group. Access can be requested by filling in the application form here. Once assigned a project code, the directory is accessible at  <code>/mnt/autofs/mcclayrds-projects/&lt;project-code&gt;</code>. </p> <p>McClayRDS is typically used for</p> <ul> <li>storage of raw data that is actively being used during a research project</li> <li>data that is required to be stored in a secure environment, replicated to a second site.</li> </ul> <p>Note</p> <p>Data in the McClayRDS is replicated to a second site to protect against data loss from disk failure/damage. The data sync occurs every 20 mins. This data is not backed up, so data loss from user modification or deletion will not be recoverable.</p>"},{"location":"Quickstart%20Guide/#storage-summary","title":"Storage Summary","text":"Description Directory Disk Space Limit File Number Limit File Deletion Policy Home <code>/users/$USER</code> 50GB 100k No automatic file deletion Scratch <code>/mnt/scratch2/users/$USER</code> N/A N/A Deletion after 90 days without access Temp <code>/tmp</code> Disk Space on Node N/A Deletion after session ends McClayRDS <code>/mnt/autofs/mcclayrds-projects/&lt;project-code&gt;</code> Project specific N/A Deletion after specified data retention period ends"},{"location":"Quickstart%20Guide/#transferring-data-with-kelvin2","title":"Transferring Data with Kelvin2","text":""},{"location":"Quickstart%20Guide/#transfer-files-using-graphical-interfaces","title":"Transfer files using graphical interfaces","text":"<p>Files can be simply transferred to and from Kelvin2 using a 'drag and drop' interface with a graphical file browser such as MobaXterm or WinSCP.</p>"},{"location":"Quickstart%20Guide/#transfer-small-files-and-folders-using-scp","title":"Transfer small files and folders using <code>scp</code>","text":"<p>Individual files can be transferred between your local computer and Kelvin2 by entering the following commands into your local terminal:</p> <p>Uploading a file</p> <p>From Inside the QUB Network: <pre><code>scp path/to/local/file.txt &lt;username&gt;@kelvin2.qub.ac.uk:/path/on/kelvin2\n</code></pre></p> <p>From Outside the QUB Network: <pre><code>scp -P 55890 -i /path/to/ssh/private/key path/to/local/file.txt &lt;username&gt;@login.kelvin.alces.network:/path/on/kelvin2\n</code></pre></p> <p>Downloading a file</p> <p>From Inside the QUB Network: <pre><code>scp &lt;username&gt;@kelvin2.qub.ac.uk:/path/to/kelvin2/file.txt path/to/local/directory/\n</code></pre></p> <p>From Outside the QUB Network: <pre><code>scp -P 55890 -i /path/to/ssh/private/key &lt;username&gt;@login.kelvin.alces.network:/path/to/kelvin2/file.txt path/to/local/directory/\n</code></pre></p> <p>Directories and their contents can also be transferred by specifing a directory instead of a file and using <code>scp</code> with a <code>-r</code> flag (for recursive).</p> <p>Note</p> <p>When using <code>scp</code>, note that the <code>-P</code> flag to specify the port is uppercase. This is a frequent cause of confusion since the flag to specify the port when using the <code>ssh</code> command is lowercase.</p>"},{"location":"Quickstart%20Guide/#transfer-large-amounts-of-data","title":"Transfer large amounts of data","text":"<p>For large amounts of data, instead of using the login nodes as shown above, please use one of the data movement nodes as this will be faster and cause less disruption to other users. The host names of these are given below:</p> <ul> <li>dm1.kelvin.alces.network</li> <li>dm2.kelvin.alces.network</li> </ul> <p>Note</p> <p>If you are transferring a large number of small files, it is recommended to first pack the files into an \"archive\" file using tools like zip or tar. For large data transfers it may also be worth looking into alternative methods such as <code>rsync</code> which allow file transfers to be stopped and restarted if required. For further support with large date transfers, please contact us.</p>"},{"location":"Running%20Jobs/","title":"Running Jobs","text":"<p>On Kelvin2, jobs are submitted via batch script to a  scheduling system called SLURM. Jobs are not necessarily run in the order in which they are submitted and those jobs which require a large number of cores, memory or walltime will have to queue until the requested resources become available. The system will run smaller jobs, that can fit in available gaps, until all of the resources that have been requested for the larger job become available.</p> <p>Note</p> <p>To improve your experience with the queue, and that of other users, please ensure that you are not requesting excessive amounts of resources to complete your jobs. After reading the below documentation, see our tips for optimising your jobscript.</p>"},{"location":"Running%20Jobs/#batch-scripts","title":"Batch Scripts","text":"<p>A SLURM batch script typically contains four main components</p> <ul> <li>A 'shebang' <code>#!</code> which states the interpreter type, i.e. <code>#!/bin/bash</code> </li> <li>A series of <code>#SBATCH</code> directives that specify resource requirements of the job and a variety of other attributes</li> <li>Lines that load modules and set the environment</li> <li>At least one executable bash line that runs code and any associated input parameters</li> </ul> <p>An brief overview of <code>#SBATCH</code> directives and modules on Kelvin2 is given below:</p>"},{"location":"Running%20Jobs/#sbatch-directives","title":"<code>#SBATCH</code> directives","text":"<p><code>#SBATCH</code> directives specify the resources required for your job and various other attributes. The definitive list of these are in the SLURM documentation, and a few of the most commonly used ones are given below.</p> <p>Attribute Directives: e.g. <code>--job-name</code>, <code>--output</code>, <code>--error</code>, <code>--time</code>, <code>--mail-type</code>, <code>--mail-user</code></p> <p><code>--job-name</code> - Specifies a name for the job. This helps identify the job when querying your queues.</p> <p><code>--output</code> - Specifies the file in which the standard output will be written. This is the output that would have been printed to the terminal, if you were running the application directly.</p> <p><code>--error</code> - Specifies the file in which the standard errors will be written.</p> <p><code>--time</code> - Specifies a time limit for the job after which it will automatically exit, e.g. 01:30:00 for 1.5 hours, or 1-00:00:00 for 1 day.  Providing an accurate time limit will help your queued jobs start running more quickly.</p> <p><code>--mail-type</code> - Notifies the user by email when various events occur, like when the job starts or ends.</p> <p><code>--mail-user</code> - Specifies the email address to receive the notifications.</p> <pre><code>#SBATCH --jobname=my-kelvin2-job\n#SBATCH --output=my-output-file\n#SBATCH --error=my-error-file\n#SBATCH --time=01:30:00\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=&lt;user-email-address&gt;\n</code></pre> <p>Partition Directive: <code>--partition</code></p> <p>The <code>--partition</code> directive should be specified according to the resource requirements and time limit of your job. For example, if you wanted to run a job with a wall time of 3 days, you would need to use a low priority partition otherwise your job would never run, e.g.</p> <pre><code>#SBATCH --partition=k2-lowpri\n#SBATCH --time=3-00:00:00\n</code></pre> <p>The main Kelvin2 partitions along with their time limits and computational resources are shown below. The default partition, if none is specified in the batch script is k2-hipri.</p> Partition Time Limit CPU Cores per Node Memory (GB) per Node k2-hipri 3 hours 128 (94 Nodes) 773 GB (59 Nodes), 1023 GB (35 Nodes) k2-medpri 24 hours \" \" k2-lowpri N/A \" \" k2-epsrc N/A \" \" k2-himem 3 days 128 (6 Nodes), 256 (2 Nodes) 2051 GB (4 Nodes) 2063 GB (4 Nodes) k2-epsrc-himem N/A \" \" k2-gpu 3 days 48 (8 Nodes), 128 (4 Nodes) 514 GB (8 Nodes), 1031GB (4 Nodes) k2-gpu-interactive 3 hours \" \" k2-epsrc-gpu 3 days \" \" <p>Other partitions are also available, including some for specific research groups. A comprehensive list of Kelvin2 nodes, their associated partitions and their computational resources can be found using the sinfo command.</p> <p>Resource Directives: e.g. <code>--ntasks</code>, <code>--nodes</code>, <code>--cpus-per-task</code>, <code>--mem-per-cpu</code></p> <p><code>#SBATCH</code> directives for computational resources follow the format</p> <pre><code>#SBATCH --&lt;resource-type&gt;=&lt;amount&gt;\n</code></pre> <p>MPI Application example</p> <p>If your MPI application requires 40 CPU cores spread across a maximum of 2 nodes and 80GB memory (2GB per CPU core) you would include the directives</p> <pre><code>#SBATCH --ntasks=40\n#SBATCH --mem-per-cpu=2G\n#SBATCH --nodes=2\n</code></pre> <p>OpenMP Application example</p> <p>If your OpenMP application require 40 CPU cores all on the same node and 80GB memory (2GB per CPU core) you would include the directives</p> <pre><code>#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=40\n#SBATCH --mem-per-cpu=2G\n#SBATCH --nodes=1\n</code></pre> <p>GPU Directives: <code>--gres</code></p> <p>Jobs submitted to the k2-gpu, k2-gpu-interactive (for interactive jobs only) and k2-epsrc-gpu partitions can access GPUs using the <code>--gres</code> flag by following the below template (<code>&lt;N&gt;</code> is the number of GPUs requested per node)</p> <pre><code>#SBATCH --gres=gpu:&lt;gpu-type&gt;:&lt;N&gt;\n</code></pre> Available GPU Resources Example <code>--gres</code> flag 32 x Tesla V100 PCIe 32Gb (4 GPUs on 8 Nodes) <code>#SBATCH --gres=gpu:v100:1</code> 12 x Tesla A100 SXM4 80GB (4 GPUs on 3 Nodes) <code>#SBATCH --gres=gpu:a100:1</code> 28 x CI slices of a Tesla A100 SXM4 80GB (7 slices on 4 GPUs on 1 Node) <code>#SBATCH --gres=gpu:1g.10gb:1</code>"},{"location":"Running%20Jobs/#loading-modules","title":"Loading Modules","text":"<p>Kelvin2 has a large repository of software installed directly on the system which can be loaded for the jobs using the batch script.</p> <p>The <code>module avail</code> command will show the list of applications that are currently installed on the central repository.</p> <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ module avail\n---  /opt/gridware/local/el7/etc/modules  ---\n  apps/anaconda/2.5.0/bin\n  apps/anaconda3/2021.05/bin\n  apps/anaconda3/2022.10/bin\n  apps/anaconda3/5.2.0/bin\n  apps/annovar/20160201/noarch\n  apps/bamtools/2.3.0/gcc-4.8.5\n  ...\n</code></pre> <p>Press <code>space</code> to navigate through the list one page at a time and when done press <code>q</code> to exit.</p> <p>The most useful commands related to working with modules are:</p> <ul> <li><code>module display &lt;module&gt;</code> -  Shows information about the software</li> <li><code>module load &lt;module&gt;</code> - load the selected module, including its binaries/libraries into the user's environment</li> <li><code>module unload &lt;module&gt;</code> - removes the module from the user's environment</li> <li><code>module list</code> - show the currently loaded modules in the user's environment</li> <li><code>module purge</code> - clear all modules from the user's environment</li> </ul>"},{"location":"Running%20Jobs/#two-batch-script-examples","title":"Two Batch Script Examples","text":"<p>For illustration, example jobscripts for CPU and GPU applications are shown below, but each should be customised for your specific job. More examples are given in the Applications section.</p> CPU Application (with OpenMPI) <pre><code>#!/bin/bash\n\n#SBATCH --job-name=mpi-job-name             # Specify a name for the job\n#SBATCH --output=mpi-job-output.out         # Specify where stdout will be written\n#SBATCH --error=mpi-job-error.err           # Specify where stderr will be written\n#SBATCH --time=01:30:00                     # time limit of 1hr30min\n#SBATCH --mail-user=&lt;email address&gt;         # Specify email address for notifications\n#SBATCH --mail-type=ALL                     # Specify types of notification (eg job Begin, End)\n#SBATCH --ntasks=32                         # Number of tasks to run (for MPI tasks this is number of cores) \n#SBATCH --nodes=4                          # Maximum number of nodes on which to launch tasks \n\n#SBATCH --partition=k2-hipri                # Specify SLURM partition to queue the job\n\n\nmodule load mpi/openmpi/4.1.1/gcc-9.3.0 # Load the application and dependencies\nmodule load &lt;mpi-application&gt;\n\nmpirun &lt;application-command&gt; -n 32 [options] # Run the code\n</code></pre> GPU Application (MATLAB) <pre><code>#!/bin/bash\n\n#SBATCH --job-name=matlab-job-name\n#SBATCH --output=matlab-output.out\n#SBATCH --time=00:30:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=4\n#SBATCH --mem-per-cpu=5G\n#SBATCH --partition=k2-gpu\n#SBATCH --gres=gpu:1g.10gb:1\n\nmodule load matlab/R2022a\n\n# Ulster University (UU) users must use UU's Matlab licence by declaring\n# (removing comments) these environment variables:\n#export MLM_LICENSE_FILE=27000@193.61.190.229\n#export LM_LICENSE_FILE=27000@193.61.190.229\n\nmatlab -nosplash -nodisplay -r \"matlab_gpu_script;\"\n</code></pre>"},{"location":"Running%20Jobs/#managing-submitted-jobs","title":"Managing Submitted Jobs","text":"<p>SLURM also has commands for managing your jobs.</p>"},{"location":"Running%20Jobs/#checking-job-status","title":"Checking job status","text":"<p>You can view the status of your submitted jobs by using the <code>squeue</code> command:</p> <p><pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ squeue -u &lt;username&gt;\n</code></pre> <pre><code>             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          &lt;jobid&gt;  k2-hipri hello-wo  &lt;username&gt;  R       0:02      1 node103\n</code></pre></p> <p>The status of your job is given by the Job State code (ST) and the nodes your job is running on (or the reason it is not running) is in the 'NODELIST(REASON)' column. Common Job State codes are:</p> State Code State Description R Running Your job is currently running. PD Pending Your job is waiting for resources to be allocated F Failed Your job submission has failed. <p>The definitive list of job state codes are given in the SLURM documentation</p>"},{"location":"Running%20Jobs/#cancelling-jobs","title":"Cancelling jobs","text":"<p>If you need to cancel a pending or running job you can use the <code>scancel</code> command: <pre><code>scancel &lt;jobid&gt;\n</code></pre></p> <p>Users can delete their own jobs only.</p>"},{"location":"Running%20Jobs/#analysing-previous-jobs-using-sacct","title":"Analysing previous jobs using <code>sacct</code>","text":"<p>Jobs you have previously ran can be analysed to check how much of the resources you had allocated were used by the program. More information may be found in the SLURM documentation.</p> <pre><code>sacct -j &lt;jobid&gt; --format=\"JobID,jobname,NTasks,nodelist,CPUTime,ReqMem,MaxVMSize,Elapsed\"\n</code></pre>"},{"location":"Running%20Jobs/#interactive-jobs","title":"Interactive Jobs","text":"<p>Interactive jobs allow the user to interact with their applications as they are running on the compute node(s). This is useful for data visualisation and for debugging programs and more complex code compilations.</p> <p>Note</p> <p>Since interactive sessions involve waiting on human input, they are generally an inefficient use of computational resources. As such, we recommend using a batch script whenever possible to run your applications automatically in the background.</p> <p>You can launch an interactive session by including the <code>srun</code> command in the terminal, followed by a list of computational resources, followed by <code>--pty bash</code>.</p> <p>For example, the follow command requests 10 CPU cores and 10GB of RAM on 1 node for a 1 hour period.</p> <p><pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ srun -p k2-hipri -N 1 -n 10 --mem-per-cpu=1G --time=1:00:00 --pty bash\n</code></pre> The job will then be queued and launched when resources are allocated on the system. Note that the domain on the terminal will change to a compute node. <pre><code>srun: job &lt;jobid&gt; queued and waiting for resources\nsrun: job &lt;jobid&gt; has been allocated resources\n[&lt;username&gt;@node103 [kelvin2] ~]$\n</code></pre></p> <p>Once connected to a login node you can load modules, set your environment and run your jobs interactively.</p>"},{"location":"Running%20Jobs/#interactive-jobs-with-graphical-applications","title":"Interactive jobs with graphical applications","text":"<p>It is possible to interact with graphical applications on Kelvin2 by launching a VNC server and creating an SSH tunnel.</p> <ol> <li> <p>Launch an interactive job on a compute node by typing the following command into your Kelvin2 terminal. Modify your resource requirements as needed.     <pre><code>[&lt;username&gt;@login3 [kelvin2] ~]$ srun -p k2-hipri -N 1 -n 10 --mem-per-cpu=1G --time=1:00:00 --pty bash\n</code></pre></p> </li> <li> <p>On the compute node, launch a vncserver (TigerVNC) by typing the following into your Kelvin2 terminal:     <pre><code>[&lt;username&gt;@node181 [kelvin2] ~]$ vncserver\n</code></pre></p> </li> <li> <p>If this is your first time launching a VNC server you will be prompted to set a password. For security reasons, this should be different to your login/SSH password for Kelvin2.     <pre><code>You will require a password to access your desktops.\n\nPassword:\nVerify:\nWould you like to enter a view-only password (y/n)? n\nA view-only password is not used\n</code></pre></p> </li> <li> <p>Once your vncserver is launched you will have information printed to your screen:     <pre><code>New 'node181.pri.kelvin2.alces.network:1 (&lt;username&gt;)' desktop is node181.pri.kelvin2.alces.network:1\n\nStarting applications specified in /users/&lt;username&gt;/.vnc/xstartup\nLog file is /users/&lt;username&gt;/.vnc/node181.pri.kelvin2.alces.network:1.log\n</code></pre></p> </li> <li> <p>Take note of the number after the compute node address (e.g. the <code>1</code> at the end of <code>node181.pri.kelvin2.alces.network:1</code>). This specifies the port number you need when creating your tunnel. Since the number here is <code>1</code>, we use port <code>5901</code>, if the number <code>7</code>, we would use port <code>5907</code>.</p> </li> <li> <p>In a separate terminal on your local computer, enter the following command to launch an SSH tunnel. Modify your node name and port numbers as appropriate. Here <code>5903</code> specifies the port on your local host and <code>5901</code> comes from the port number noted in the previous step. You will have to log in and authenticate using your usual credentials.</p> <p>From inside the QUB Network <pre><code>ssh -L 5903:node181.pri.kelvin2.alces.network:5901 &lt;username&gt;@kelvin2.qub.ac.uk\n</code></pre></p> <p>From outside the QUB Network <pre><code>ssh -L 5903:node181.pri.kelvin2.alces.network:5901 -p 55890 -i /path/to/ssh/key &lt;username&gt;@login.kelvin.alces.network\n</code></pre></p> </li> <li> <p>Now open  your VNC application and connect to localhost on port 5903 (or whichever port you chose in Step 6). You will have to enter the password set on Step 3.</p> </li> <li> <p>You will now have a graphical view of a terminal in Kelvin2. From here you may load modules and open graphical applications as required.</p> </li> <li> <p>After you have finished your session</p> <ul> <li>disconnect from your graphical session on your VNC application.</li> <li> <p>find and close the vncserver on the compute node by typing the following commands into your Kelvin2 terminal.</p> <p><pre><code>[&lt;username&gt;@node181 [kelvin2] ~]$ vncserver -list\n\nTigerVNC server sessions:\n\nX DISPLAY #     PROCESS ID\n:1              102389\n[&lt;username&gt;@node181 [kelvin2] ~]$ vncserver -kill :1\nKilling Xvnc process ID 102389\n</code></pre>     - Close the tunnel by exiting the local terminal opened in Step 6.</p> </li> </ul> </li> </ol>"},{"location":"Running%20Jobs/#tips-for-optimising-your-jobscript","title":"Tips for optimising your jobscript","text":"<ul> <li>Use the job analysis tool sacct to check if you allocated more memory than the necessary for futures jobs with the same application, e.g. <code>sacct -j &lt;job_num&gt; --format=\"JobID,jobname,NTasks,nodelist,CPUTime,ReqMem,MaxVMSize,Elapsed\"</code></li> <li>Do not allocate more resources than available in the requested nodes or you job will not run.</li> <li>When possible, try not to allocate a large number of CPUs or a large amount of memory in a single node. Instead, spread the allocation among several nodes. 20 CPUs and 100 GB of memory is a reasonable amount to be allocated in a single node.</li> <li>Give freedom to Slurm to distribute the resources among the nodes. Use the flags <code>--ntasks</code> and <code>--mem-per-cpu</code> to allocate resources per CPU, rather than <code>--ntasks-per-node</code> or <code>--mem</code>.</li> <li>Allocate always the correct partition, double check the resources you require, in particular the wall time, and fit it in the particular partition.</li> <li>Specify an error output different to the standard output. In case the job crashes, this output has useful information about why the job failed, so how it can be fix.</li> <li>Activate email notifications.</li> <li>If you are intending to run a large number of similar jobs, consider submitting a Job Array</li> </ul>"},{"location":"Running%20Jobs/#slurm-cheat-sheets","title":"Slurm cheat sheets","text":"Common job commands Description Command Submit a job sbatch  Delete a job scancel  Job status (all) squeue Job status (by job) squeue -j  Job status (detailed) scontrol show job  Show expected start time squeue -j  --start Start an interactive job srun --pty  /bin/bash Monitor jobs resource usage sacct -j  --format=\"JobID,jobname,NTasks,nodelist,CPUTime,ReqMem,MaxVMSize,Elapsed\" Slurm Environmental variables Variable Function SLURM_ARRAY_JOB_ID Job array's master job ID number. SLURM_ARRAY_TASK_ID Job array ID (index) number. SLURM_CLUSTER_NAME Name of the cluster on which the job is executing. SLURM_CPUS_PER_TASK Number of cpus requested per task. Only set if the --cpus-per-task option is specified. SLURM_JOB_ACCOUNT Account name associated of the job allocation. SLURM_JOB_ID The ID of the job allocation. SLURM_JOB_NAME Name of the job. SLURM_JOB_NODELIST List of nodes allocated to the job. SLURM_JOB_NUM_NODES Total number of nodes in the job's resource allocation. SLURM_JOB_PARTITION Name of the partition in which the job is running. SLURM_JOB_UID The ID of the job allocation. See SLURM_JOB_ID. Included for backwards compatibility. SLURM_JOB_USER User name of the job owner SLURM_MEM_PER_CPU Same as --mem-per-cpu SLURM_MEM_PER_NODE Same as --mem SLURM_NTASKS Same as -n, --ntasks SLURM_NTASKS_PER_NODE Number of tasks requested per node. Only set if the --ntasks-per-node option is specified. SLURM_PROCID The MPI rank (or relative process ID) of the current process. <p>More information about  Slurm commands, flags and environment variables, can be found in the  Slurm web page.</p>"}]}